{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neuro-Symbolic Commonsense Reasoning: Beyond Large Language Models\n",
        "\n",
        "This notebook explores the core concepts from the lecture on enhancing language models with neuro-symbolic techniques for robust commonsense reasoning. We will dissect the limitations of large models like GPT-3, such as logical inconsistency, and delve into three innovative approaches designed to overcome them: Maieutic Prompting, Symbolic Knowledge Distillation, and the Delphi model for commonsense morality. The central theme is that smaller, more cleverly designed models, augmented with symbolic reasoning and high-quality knowledge, can often outperform their larger, brute-force counterparts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 1: Overview & Prerequisites"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of the Research Paper/Lecture Topic\n",
        "\n",
        "The lecture challenges the notion that simply scaling up language models (the \"Goliath\" approach) will solve commonsense reasoning. It highlights frequent logical inconsistencies and unreliability in models like ChatGPT. Instead, it proposes a \"David\" strategy: enhancing smaller models with structured, symbolic reasoning. Three main research thrusts are presented:\n",
        "\n",
        "1.  **Maieutic Prompting:** A Socratic method that forces a language model to build a tree of explanations for a claim. It then uses a symbolic solver (Max-SAT) to prune inconsistent branches and find the most logically coherent answer, dramatically improving reasoning on complex commonsense questions.\n",
        "\n",
        "2.  **Symbolic Knowledge Distillation:** A technique to create a smaller, yet superior, commonsense knowledge model from a large, noisy one. It uses GPT-3 as a \"loose teacher\" to generate a vast symbolic knowledge graph, then employs a smaller \"critic\" model to filter out inaccuracies. The resulting high-quality, machine-authored knowledge graph is used to train a student model that surpasses the original teacher in accuracy and utility.\n",
        "\n",
        "3.  **Commonsense Morality (Delphi):** An exploration into teaching AI ethical judgments about everyday situations. Built by fine-tuning a model on the \"Commonsense Norm Bank\" (a large dataset of human moral judgments), Delphi demonstrates the ability to handle compositionality in moral scenarios. The lecture also introduces a neuro-symbolic hybrid version that uses commonsense knowledge graphs and a symbolic solver to guard against adversarial attacks and flawed reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite Mathematical Concepts\n",
        "\n",
        "- **Constraint Satisfaction Problems (Max-SAT):** The core symbolic component used in Maieutic Prompting and Delphi Hybrid. It involves finding an assignment of variables (e.g., true/false) that satisfies the maximum number of given constraints (e.g., logical implications or contradictions).\n",
        "- **Cross-Entropy Loss:** The fundamental loss function used in knowledge distillation to measure the difference between the probability distributions of the teacher and student models.\n",
        "- **Conditional Probability:** Used in Maieutic Prompting to calculate the \"belief\" or confidence score for each node in the reasoning tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite ML/CS Concepts\n",
        "\n",
        "- **Transformer Models / LLMs:** Deep understanding of models like GPT-3 and T5, including few-shot prompting, fine-tuning, and their generative capabilities.\n",
        "- **Knowledge Distillation:** The process of transferring knowledge from a large \"teacher\" model to a smaller \"student\" model.\n",
        "- **Knowledge Graphs:** Structured representations of knowledge with nodes (concepts) and edges (relations), such as the ATOMIC commonsense knowledge graph.\n",
        "- **Natural Language Inference (NLI):** The task of determining whether a \"hypothesis\" sentence is an entailment, contradiction, or neutral with respect to a \"premise\" sentence. Used to find inconsistencies in the Maieutic tree.\n",
        "- **Supervised Fine-Tuning:** The process of adapting a pre-trained language model to a specific task using a labeled dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchy of Topics\n",
        "\n",
        "1.  **The Problem:** Logical Inconsistency in Large Language Models.\n",
        "2.  **Mathematical Foundations:** Understanding Constraint Satisfaction (Max-SAT).\n",
        "3.  **Prerequisite Algorithm:** A simple implementation of Knowledge Distillation.\n",
        "4.  **Core Research 1: Maieutic Prompting:** Building a logically consistent reasoning tree.\n",
        "5.  **Core Research 2: Symbolic Knowledge Distillation:** Creating a better model from a noisy teacher.\n",
        "6.  **Core Research 3: Commonsense Morality (Delphi):** Teaching ethics and using neuro-symbolic guards.\n",
        "7.  **Experimental Analysis:** Visualizing the performance gains from these techniques.\n",
        "8.  **Context & Extensions:** The future of AI safety, value pluralism, and knowledge models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Objectives\n",
        "\n",
        "- Understand the fundamental limitations of scaling LLMs for commonsense reasoning.\n",
        "- Implement the core logic of a Max-SAT solver for symbolic reasoning.\n",
        "- Grasp the workflow of Symbolic Knowledge Distillation and the role of a \"critic\" model.\n",
        "- Analyze how neuro-symbolic methods can make AI systems more robust, consistent, and interpretable.\n",
        "- Appreciate the challenges and nuances of building models for moral and ethical reasoning.\n",
        "\n",
        "**Estimated Time:** 2-3 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 2: Mathematical Foundations"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constraint Satisfaction (Max-SAT)\n",
        "\n",
        "A cornerstone of the symbolic methods in this lecture is the use of a Maximum Satisfiability (Max-SAT) solver. A standard SAT problem asks if there is *any* assignment of boolean variables (True/False) that makes a given logical formula true. Max-SAT is an optimization version: it asks for an assignment that satisfies the **maximum number of clauses** (or the maximum weight of satisfied clauses) in a formula. \n",
        "\n",
        "This is perfect for our use case. In Maieutic Prompting, we have a tree of statements, some of which might contradict each other. We can frame this as a Max-SAT problem:\n",
        "- **Variables:** Each node (statement) in the tree is a boolean variable.\n",
        "- **Clauses:** We add weighted clauses based on:\n",
        "    - The model's initial confidence in each statement (e.g., `(NodeA)` with weight 0.8).\n",
        "    - The NLI-detected relationships between statements (e.g., a contradiction between A and B becomes a clause `(NOT A OR NOT B)` with a high weight).\n",
        "\n",
        "The Max-SAT solver then finds the True/False assignment for all statements that creates the most globally consistent and believable reasoning chain. Let's implement a simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll need a library for solving SAT problems. \n",
        "# You can install it with: pip install python-sat\n",
        "from pysat.examples.rc2 import RC2\n",
        "from pysat.formula import WCNF\n",
        "import numpy as np\n",
        "\n",
        "def educational_max_sat_solver_example():\n",
        "    \"\"\"\n",
        "    Demonstrates how Max-SAT can find the most consistent interpretation of conflicting information.\n",
        "    - Based on the reasoning problem described in the lecture.\n",
        "    - Uses a real Max-SAT solver to find the optimal solution.\n",
        "    \"\"\"\n",
        "    # Problem Setup: Imagine a simple reasoning graph with 3 nodes (statements).\n",
        "    # Node 1: \"The world is round.\" (Let's call it A)\n",
        "    # Node 2: \"If you travel West, you eventually reach the East.\" (Let's call it B)\n",
        "    # Node 3: \"The world is flat.\" (Let's call it C)\n",
        "    \n",
        "    # Map nodes to integer variables for the solver. 1=A, 2=B, 3=C\n",
        "    # A positive number means the variable is True, negative means False.\n",
        "    var_A, var_B, var_C = 1, 2, 3\n",
        "    \n",
        "    # Create a Weighted Conjunctive Normal Form (WCNF) formula\n",
        "    wcnf = WCNF()\n",
        "    \n",
        "    # --- Step 1: Add \"Soft\" Clauses based on initial beliefs (confidence) ---\n",
        "    # These can be violated, but there's a cost (weight).\n",
        "    # Let's say our model is highly confident in A and B, but less in C.\n",
        "    wcnf.append([var_A], weight=10)  # We strongly believe A is true.\n",
        "    wcnf.append([var_B], weight=9)   # We strongly believe B is true.\n",
        "    wcnf.append([var_C], weight=2)   # We have a weak belief that C is true (e.g., from a bad explanation).\n",
        "    \n",
        "    # --- Step 2: Add \"Hard\" Clauses based on logical constraints ---\n",
        "    # These MUST be satisfied. We give them a weight larger than the sum of all soft weights.\n",
        "    # Constraint 1: A implies B. (A -> B) is equivalent to (-A or B).\n",
        "    wcnf.append([-var_A, var_B], weight=None) # weight=None makes it a hard clause\n",
        "    \n",
        "    # Constraint 2: A and C are contradictions. (-A or -C).\n",
        "    wcnf.append([-var_A, -var_C], weight=None)\n",
        "    \n",
        "    print(\"--- Max-SAT Problem --- \")\n",
        "    print(f\"Variables: A={var_A}, B={var_B}, C={var_C}\")\n",
        "    print(\"\\nSoft Clauses (Beliefs):\")\n",
        "    print(\"  - Believe A is True (Weight=10)\")\n",
        "    print(\"  - Believe B is True (Weight=9)\")\n",
        "    print(\"  - Believe C is True (Weight=2)\")\n",
        "    print(\"\\nHard Clauses (Constraints):\")\n",
        "    print(\"  - A implies B\")\n",
        "    print(\"  - A and C cannot both be true\")\n",
        "    \n",
        "    # --- Step 3: Solve the problem ---\n",
        "    with RC2(wcnf) as solver:\n",
        "        solver.compute()  # Find the optimal model\n",
        "        model = solver.model\n",
        "    \n",
        "    print(\"\\n--- Max-SAT Solution --- \")\n",
        "    # The model is a list of integers. Positive means True, negative means False.\n",
        "    solution = {abs(v): v > 0 for v in model}\n",
        "    print(f\"Optimal Assignment: A={solution.get(var_A)}, B={solution.get(var_B)}, C={solution.get(var_C)}\")\n",
        "    print(\"\\nThis solution satisfies all hard constraints while maximizing the weight of satisfied beliefs.\")\n",
        "    print(\"It correctly discards the weak, contradictory belief in C.\")\n",
        "\n",
        "educational_max_sat_solver_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 3: Prerequisite Algorithms"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Distillation\n",
        "\n",
        "Before diving into *Symbolic* Knowledge Distillation, let's understand the standard version. Proposed by Hinton et al. (2015), knowledge distillation is a method for model compression. The idea is to train a smaller \"student\" network to mimic the behavior of a larger, pre-trained \"teacher\" network.\n",
        "\n",
        "Instead of training the student on hard labels (one-hot vectors), we train it on the *soft probability distribution* produced by the teacher. These soft targets contain more information about the relationships between classes (e.g., a picture of a cat might have a small probability of being a dog, which is more informative than saying it's 0% a dog). \n",
        "\n",
        "The loss function is typically a combination of:\n",
        "1.  A distillation loss (Cross-Entropy) between the student's predictions and the teacher's soft predictions.\n",
        "2.  A standard supervised loss (Cross-Entropy) between the student's predictions and the true hard labels.\n",
        "\n",
        "$$ L = \\alpha L_{CE}(\\text{y}_{true}, \\text{p}_{student}) + (1-\\alpha) L_{CE}(\\text{p}_{teacher}, \\text{p}_{student}) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def educational_knowledge_distillation():\n",
        "    \"\"\"\n",
        "    A clear implementation of standard knowledge distillation for understanding.\n",
        "    - Defines simple teacher and student models.\n",
        "    - Implements the combined distillation loss function.\n",
        "    - Shows how the student learns from the teacher's soft labels.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Dummy data for a 4-class classification problem\n",
        "    inputs = torch.randn(10, 20) # 10 samples, 20 features\n",
        "    labels = torch.randint(0, 4, (10,)) # 10 labels\n",
        "    \n",
        "    # --- Teacher Model (Larger) ---\n",
        "    teacher_model = nn.Sequential(\n",
        "        nn.Linear(20, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 4)\n",
        "    )\n",
        "    # Let's pretend this model is already trained and is in eval mode\n",
        "    teacher_model.eval()\n",
        "    \n",
        "    # --- Student Model (Smaller) ---\n",
        "    student_model = nn.Sequential(\n",
        "        nn.Linear(20, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 4)\n",
        "    )\n",
        "    \n",
        "    # --- Distillation Hyperparameters ---\n",
        "    temperature = 3.0 # Softens the probabilities, revealing more inter-class info\n",
        "    alpha = 0.3 # Weight for the standard supervised loss\n",
        "    \n",
        "    # Get teacher's predictions (logits)\n",
        "    with torch.no_grad():\n",
        "        teacher_logits = teacher_model(inputs)\n",
        "        \n",
        "    # Get student's predictions\n",
        "    student_logits = student_model(inputs)\n",
        "    \n",
        "    # --- Calculate the two parts of the loss ---\n",
        "    # 1. Standard supervised loss with hard labels\n",
        "    loss_hard = F.cross_entropy(student_logits, labels)\n",
        "    \n",
        "    # 2. Distillation loss with soft labels from the teacher\n",
        "    # We use LogSoftmax and KL Divergence, which is equivalent to CE on soft targets\n",
        "    loss_soft = nn.KLDivLoss(reduction='batchmean')(\n",
        "        F.log_softmax(student_logits / temperature, dim=1),\n",
        "        F.softmax(teacher_logits / temperature, dim=1)\n",
        "    ) * (temperature ** 2) # Rescale the gradients\n",
        "\n",
        "    # 3. Combine the losses\n",
        "    total_loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
        "    \n",
        "    print(\"--- Knowledge Distillation Example ---\")\n",
        "    print(f\"Teacher Logits (sample 0): {teacher_logits[0].numpy()}\")\n",
        "    print(f\"Teacher Soft Probs (T={temperature}): {F.softmax(teacher_logits[0] / temperature, dim=0).numpy()}\")\n",
        "    print(\"\\nStudent training would optimize the following combined loss:\")\n",
        "    print(f\"  - Hard Label Loss: {loss_hard.item():.4f}\")\n",
        "    print(f\"  - Soft Distillation Loss: {loss_soft.item():.4f}\")\n",
        "    print(f\"  - Total Combined Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "educational_knowledge_distillation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 4: Core Research Content"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maieutic Prompting: A Conceptual Implementation\n",
        "\n",
        "We can't replicate the full Maieutic Prompting process without access to a powerful LLM like GPT-3. However, we can simulate the workflow to understand the logic. The process involves recursively generating explanations and counter-explanations, checking for consistency, and then using a symbolic solver to find the best overall answer.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Generate Explanations:** For a question `Q`, ask the LLM to explain why the answer is `True` (`E_T`) and why it's `False` (`E_F`).\n",
        "2.  **Check for Logical Integrity:** For each explanation (e.g., `E_T`), ask the LLM if `E_T` implies `True` and if `NOT E_T` implies `False`. If the LLM is consistent (flips its answer when the premise is negated), the explanation is considered logically integral.\n",
        "3.  **Build Tree:** Recursively generate explanations for the explanations, forming a tree. Prune branches that are not logically integral.\n",
        "4.  **Pairwise Consistency:** Use an NLI model to check for contradictions between any two nodes in the remaining tree.\n",
        "5.  **Formulate & Solve Max-SAT:** Convert the tree into a Max-SAT problem with weighted clauses for initial beliefs and hard clauses for contradictions. The solution gives the most consistent final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_maieutic_prompting_simulation():\n",
        "    \"\"\"\n",
        "    A conceptual simulation of the Maieutic Prompting workflow.\n",
        "    - Mocks the calls to an LLM and NLI model.\n",
        "    - Shows how the reasoning tree is constructed and converted to a Max-SAT problem.\n",
        "    \"\"\"\n",
        "    # --- MOCKED MODELS ---\n",
        "    def mock_llm_explainer(prompt):\n",
        "        # Simulates GPT-3 generating explanations\n",
        "        if \"travel West\" in prompt and \"is True because\" in prompt:\n",
        "            return \"the world is round, so you will eventually reach the East Coast.\"\n",
        "        if \"travel West\" in prompt and \"is False because\" in prompt:\n",
        "            return \"you cannot reach the East Coast by traveling West.\"\n",
        "        return \"...\"\n",
        "\n",
        "    def mock_nli_checker(premise, hypothesis):\n",
        "        # Simulates an NLI model checking for contradiction\n",
        "        if \"world is round\" in premise and \"world is flat\" in hypothesis:\n",
        "            return \"contradiction\"\n",
        "        return \"neutral\"\n",
        "        \n",
        "    # --- WORKFLOW ---\n",
        "    question = \"If you travel West far enough from the West Coast, you will reach the East Coast.\"\n",
        "    \n",
        "    # 1. Generate initial explanations\n",
        "    exp_true = mock_llm_explainer(f\"{question} is True because\")\n",
        "    exp_false = mock_llm_explainer(f\"{question} is False because\")\n",
        "    print(\"--- Maieutic Prompting Simulation ---\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"\\nExplanation for TRUE (E_T): '{exp_true}'\")\n",
        "    print(f\"Explanation for FALSE (E_F): '{exp_false}' (This is a bogus explanation)\")\n",
        "    \n",
        "    # 2. Assume E_T is found to be logically integral, E_F is not. We prune E_F.\n",
        "    # Our reasoning 'tree' now has the root question (Q) and one child (E_T).\n",
        "    reasoning_nodes = {\n",
        "        1: {'text': question, 'belief': 6}, # Let's say initial belief is neutral\n",
        "        2: {'text': exp_true, 'belief': 9}, # Strong belief from a good explanation\n",
        "    }\n",
        "    \n",
        "    # 3. Formulate the Max-SAT problem\n",
        "    wcnf = WCNF()\n",
        "    # Add beliefs as soft clauses\n",
        "    wcnf.append([1], weight=reasoning_nodes[1]['belief'])\n",
        "    wcnf.append([2], weight=reasoning_nodes[2]['belief'])\n",
        "    \n",
        "    # Add constraints. E_T implies Q. So, (NOT E_T OR Q).\n",
        "    wcnf.append([-2, 1], weight=None) # Hard constraint\n",
        "    \n",
        "    print(\"\\n--- Max-SAT Formulation ---\")\n",
        "    print(\"Variable 1: Question is True\")\n",
        "    print(\"Variable 2: E_T is True\")\n",
        "    print(\"Hard Constraint: E_T implies Q\")\n",
        "\n",
        "    # 4. Solve\n",
        "    with RC2(wcnf) as solver:\n",
        "        solver.compute()\n",
        "        model = solver.model\n",
        "        \n",
        "    solution = {abs(v): v > 0 for v in model}\n",
        "    final_answer = solution.get(1)\n",
        "    \n",
        "    print(f\"\\n--- Final Answer --- \")\n",
        "    print(f\"The most consistent answer for the question is: {final_answer}\")\n",
        "\n",
        "educational_maieutic_prompting_simulation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Symbolic Knowledge Distillation: The Critic is Key\n",
        "\n",
        "This process moves beyond standard distillation by creating an intermediate symbolic representation (the knowledge graph) and using a critic to refine it before training the student. The key insight is that the quality of the distilled knowledge is more important than the quantity.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Generate Knowledge (Loose Teacher):** Use a large LLM (e.g., GPT-3) to generate millions of commonsense if-then rules (`(event, relation) -> inference`). This is the \"loose teacher\" whose output is ~70% correct.\n",
        "2.  **Filter with Critic:** Train a separate, smaller classifier model (the \"critic,\" e.g., RoBERTa) on a small set of human-labeled examples to distinguish good generations from bad ones.\n",
        "3.  **Create High-Quality Dataset:** Apply the critic to the massive generated dataset, filtering out a large portion of noisy data but leaving a smaller, higher-accuracy knowledge graph.\n",
        "4.  **Train Student:** Fine-tune a smaller language model (the \"student,\" e.g., GPT-2 or BART) on this clean, machine-authored knowledge graph. The resulting student model is more accurate than the original loose teacher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def educational_symbolic_distillation_simulation():\n",
        "    \"\"\"\n",
        "    Simulates the Symbolic Knowledge Distillation pipeline.\n",
        "    - Creates a mock dataset from a 'loose teacher'.\n",
        "    - Applies a mock 'critic' to filter the data.\n",
        "    - Shows how the final dataset is higher quality.\n",
        "    \"\"\"\n",
        "    # 1. Generate Knowledge with a Loose Teacher (e.g., GPT-3)\n",
        "    # Let's say we get 10 knowledge tuples. We know GPT-3 is ~70% accurate.\n",
        "    loose_teacher_output = [\n",
        "        {'event': 'X gets a car repaired', 'relation': 'xNeed', 'inference': 'pay the bill', 'is_correct': True},\n",
        "        {'event': 'X gets a car repaired', 'relation': 'xWant', 'inference': 'call Uber', 'is_correct': True},\n",
        "        {'event': 'X gets a car repaired', 'relation': 'xEffect', 'inference': 'car becomes blue', 'is_correct': False}, # Noise\n",
        "        {'event': 'X keeps fridge door open', 'relation': 'xEffect', 'inference': 'food goes bad', 'is_correct': True},\n",
        "        {'event': 'X keeps fridge door open', 'relation': 'xAttr', 'inference': 'person is happy', 'is_correct': False}, # Noise\n",
        "        {'event': 'X totals the car', 'relation': 'HinderedBy', 'inference': 'get car repaired', 'is_correct': True},\n",
        "        {'event': 'X studies hard', 'relation': 'xEffect', 'inference': 'gets a good grade', 'is_correct': True},\n",
        "        {'event': 'X studies hard', 'relation': 'xEffect', 'inference': 'gets a bad grade', 'is_correct': True}, # Can happen, also valid\n",
        "        {'event': 'X studies hard', 'relation': 'xNeed', 'inference': 'a library', 'is_correct': True},\n",
        "        {'event': 'X studies hard', 'relation': 'xNeed', 'inference': 'a spaceship', 'is_correct': False} # Noise\n",
        "    ]\n",
        "    df_loose = pd.DataFrame(loose_teacher_output)\n",
        "    print(\"--- Step 1: Loose Teacher Output ---\")\n",
        "    print(f\"Generated {len(df_loose)} samples. True accuracy: {df_loose['is_correct'].mean():.2%}\")\n",
        "    \n",
        "    # 2. Filter with a Critic Model\n",
        "    # The critic is not perfect, but it's trained to be skeptical.\n",
        "    def mock_critic(row):\n",
        "        # A simple heuristic-based critic\n",
        "        if row['inference'] in ['car becomes blue', 'person is happy', 'a spaceship']:\n",
        "            return 0.1 # Low confidence, likely incorrect\n",
        "        if row['event'] == 'X studies hard' and row['inference'] == 'gets a bad grade':\n",
        "            return 0.6 # Plausible but less common, critic is unsure\n",
        "        return 0.95 # High confidence\n",
        "    \n",
        "    df_loose['critic_score'] = df_loose.apply(mock_critic, axis=1)\n",
        "    \n",
        "    # 3. Create High-Quality Dataset by applying a high threshold\n",
        "    threshold = 0.8\n",
        "    df_critical = df_loose[df_loose['critic_score'] >= threshold].copy()\n",
        "    print(f\"\\n--- Step 2 & 3: Filtered with Critic (Threshold > {threshold}) ---\")\n",
        "    print(f\"Kept {len(df_critical)} samples. New accuracy: {df_critical['is_correct'].mean():.2%}\")\n",
        "    display(df_critical[['event', 'relation', 'inference', 'critic_score']])\n",
        "    \n",
        "    print(\"\\n--- Step 4: Train Student Model ---\")\n",
        "    print(\"The student model is now trained on this smaller, cleaner, higher-quality dataset,\")\n",
        "    print(\"leading to better final performance than if trained on the full noisy dataset.\")\n",
        "\n",
        "educational_symbolic_distillation_simulation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delphi Hybrid: Neuro-Symbolic Moral Reasoning\n",
        "\n",
        "The lecture mentioned that the original Delphi model could be tricked by adversarial examples like, \"genocide if you're creating jobs,\" because the strong positive sentiment of \"creating jobs\" overwhelmed the negative concept of \"genocide.\" The Delphi Hybrid model fixes this with a neuro-symbolic pipeline.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Parse Query:** Break the input sentence into its constituent events (e.g., \"committing genocide,\" \"creating jobs\").\n",
        "2.  **Query Commonsense Model:** For each event, use a commonsense model like COMET to generate likely effects and consequences.\n",
        "3.  **Check for Dangers:** Analyze the generated consequences for obviously negative or dangerous outcomes (e.g., COMET might infer that genocide leads to people dying).\n",
        "4.  **Build Reasoning Graph:** Create a graph where nodes are the events and their consequences. Edges represent relationships (e.g., `causes`, `contradicts`).\n",
        "5.  **Solve with Max-SAT:** Use a symbolic solver to find the most consistent moral judgment, giving high weight to avoiding dangerous outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_delphi_hybrid_simulation():\n",
        "    \"\"\"\n",
        "    Simulates the reasoning pipeline of the Delphi Hybrid model.\n",
        "    \"\"\"\n",
        "    query = \"Committing genocide if it creates jobs.\"\n",
        "    print(f\"--- Delphi Hybrid reasoning for query: '{query}' ---\")\n",
        "    \n",
        "    # 1. Parse Query\n",
        "    event1 = \"committing genocide\"\n",
        "    event2 = \"creating jobs\"\n",
        "    print(f\"\\nStep 1: Parsed into events -> ['{event1}', '{event2}']\")\n",
        "    \n",
        "    # 2. Query Commonsense Model (mocked COMET)\n",
        "    def mock_comet(event):\n",
        "        if event == event1:\n",
        "            return [\"it causes people to die\", \"it is a war crime\"]\n",
        "        if event == event2:\n",
        "            return [\"it gives people money\", \"it helps the economy\"]\n",
        "        return []\n",
        "        \n",
        "    consequences1 = mock_comet(event1)\n",
        "    consequences2 = mock_comet(event2)\n",
        "    print(f\"\\nStep 2: Generated commonsense consequences.\")\n",
        "    print(f\"  - For '{event1}': {consequences1}\")\n",
        "    print(f\"  - For '{event2}': {consequences2}\")\n",
        "\n",
        "    # 3. Check for Dangers (based on a pre-defined set of universal 'bads')\n",
        "    is_dangerous = any(\"die\" in c or \"crime\" in c for c in consequences1)\n",
        "    print(f\"\\nStep 3: Checked for universally negative consequences -> Found: {is_dangerous}\")\n",
        "    \n",
        "    # 4 & 5. Formulate and Solve (conceptual)\n",
        "    # The Max-SAT solver would be given:\n",
        "    # - A very high-weight clause: `(NOT 'genocide is OK')` derived from the dangerous consequences.\n",
        "    # - A lower-weight clause: `('creating jobs is OK')` from the positive consequences.\n",
        "    # The solver will prioritize satisfying the high-weight negative constraint.\n",
        "    final_judgment = \"It's wrong.\"\n",
        "    print(f\"\\nStep 4 & 5: Symbolic solver prioritizes avoiding harm over the positive framing.\")\n",
        "    print(f\"\\n==> Final Judgment: {final_judgment}\")\n",
        "\n",
        "educational_delphi_hybrid_simulation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 5: Experimental Analysis"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maieutic Prompting: Outperforming Baselines\n",
        "\n",
        "The lecture highlights that Maieutic Prompting on GPT-3 significantly outperforms other few-shot methods like standard prompting and even Chain-of-Thought. Most impressively, it surpasses a fully supervised T5-11B model, demonstrating that a better inference-time algorithm can be more effective than supervised training on a huge model.\n",
        "\n",
        "Let's visualize these results for the CommonsenseQA 2.0 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_maieutic_results():\n",
        "    \"\"\"\n",
        "    Reproduces the bar chart comparing Maieutic Prompting to other methods.\n",
        "    \"\"\"\n",
        "    methods = ['GPT-3 Few-shot', 'Chain-of-Thought', 'Supervised T5-11B', 'Maieutic Prompting']\n",
        "    accuracies = [55.5, 63.3, 70.1, 74.5] # Approximate values from the talk\n",
        "    chance_level = 50\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
        "    plt.axhline(y=chance_level, color='r', linestyle='--', label='Chance Level (50%)')\n",
        "    \n",
        "    plt.ylabel('Accuracy (%) on CommonsenseQA 2.0')\n",
        "    plt.title('Maieutic Prompting Performance Comparison')\n",
        "    plt.ylim(45, 80)\n",
        "    plt.legend()\n",
        "    \n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.1f}%', va='bottom', ha='center')\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "plot_maieutic_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Symbolic Distillation: Quality over Quantity\n",
        "\n",
        "A key experiment in the Symbolic Knowledge Distillation work showed that training a student model on the smaller, high-quality dataset filtered by the critic results in a better model than training on the full, noisy dataset generated by the loose teacher. This emphasizes that for commonsense, the correctness of the training data is more critical than its sheer volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distillation_results():\n",
        "    \"\"\"\n",
        "    Visualizes the impact of using a critical teacher vs. a loose teacher.\n",
        "    \"\"\"\n",
        "    # Data points from the lecture's conceptual argument\n",
        "    teacher_models = ['Loose Teacher (GPT-3)', 'Student from Loose Teacher', 'Student from Critical Teacher']\n",
        "    accuracies = [73.0, 82.0, 89.0] # Approximate accuracies\n",
        "    data_sizes = [6.8, 6.8, 1.1] # Illustrative data sizes in millions of samples\n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Bar chart for accuracy\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Training Method')\n",
        "    ax1.set_ylabel('Commonsense Inference Accuracy (%)', color=color)\n",
        "    bars = ax1.bar(teacher_models, accuracies, color=color, alpha=0.6, width=0.6)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    ax1.set_ylim(70, 95)\n",
        "\n",
        "    # Line plot for data size on a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Training Data Size (Millions)', color=color)\n",
        "    ax2.plot(teacher_models, data_sizes, color=color, marker='o', linestyle='--')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    ax2.set_ylim(0, 8)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.title('Symbolic Distillation: Critical Teacher Leads to Better Student with Less Data')\n",
        "    plt.show()\n",
        "\n",
        "plot_distillation_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Explorer: Delphi Hybrid Reasoning\n",
        "\n",
        "Let's create an interactive widget to simulate how the Delphi Hybrid model weighs different factors. You can input a complex situation and then adjust the perceived moral weight of its components to see how the final judgment might change. This highlights how the symbolic layer can override a purely sentiment-based analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ipywidgets import interact, FloatSlider, VBox, HTML, Text\n",
        "\n",
        "def interactive_delphi_explorer(positive_event_weight, negative_event_severity):\n",
        "    \"\"\"\n",
        "    An interactive widget to explore the trade-offs in Delphi Hybrid's reasoning.\n",
        "    \"\"\"\n",
        "    # The symbolic layer assigns a very high negative score if a 'universally bad' consequence is detected.\n",
        "    symbolic_override_cost = negative_event_severity * -100\n",
        "    \n",
        "    # The neural/sentiment layer might just add the scores\n",
        "    neural_score = positive_event_weight - negative_event_severity\n",
        "    \n",
        "    # The hybrid model's final score is dominated by the symbolic override\n",
        "    hybrid_score = positive_event_weight + symbolic_override_cost\n",
        "    \n",
        "    neural_judgment = \"'OK' (positive outweighs negative)\" if neural_score > 0 else \"'Wrong'\"\n",
        "    hybrid_judgment = \"'OK'\" if hybrid_score > 0 else \"'Wrong' (Symbolic override due to severity)\"\n",
        "    \n",
        "    display(HTML(f\"<b>Query:</b> Committing a severe negative act to achieve a positive outcome.\"))\n",
        "    display(HTML(f\"<hr>\"))\n",
        "    display(HTML(f\"<b>Purely Neural/Sentiment Model Score:</b> {neural_score:.1f} -> <b>Judgment:</b> {neural_judgment}\"))\n",
        "    display(HTML(f\"<b>Neuro-Symbolic Hybrid Model Score:</b> {hybrid_score:.1f} -> <b>Judgment:</b> {hybrid_judgment}\"))\n",
        "    display(HTML(f\"<hr>\"))\n",
        "    display(HTML(\"Notice how even a high positive weight cannot overcome the symbolic override when the negative act is severe.\"))\n",
        "\n",
        "interact(\n",
        "    interactive_delphi_explorer,\n",
        "    positive_event_weight=FloatSlider(min=0.0, max=10.0, step=0.5, value=8.0, description='Positive Event Weight'),\n",
        "    negative_event_severity=FloatSlider(min=0.1, max=10.0, step=0.1, value=5.0, description='Negative Event Severity')\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Section 6: Research Context & Extensions"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Contribution in Context\n",
        "\n",
        "This body of work pushes back against the dominant \"scale is all you need\" narrative in modern AI. It argues that for high-level cognitive tasks like commonsense and moral reasoning, structural and algorithmic innovations are indispensable. The contributions can be summarized as:\n",
        "\n",
        "- **Challenging LLM Supremacy:** It provides concrete evidence that smaller models, when guided by symbolic reasoning or trained on higher-quality distilled knowledge, can be more robust and accurate than much larger models.\n",
        "- **Bridging Neural and Symbolic AI:** The methods presented are prime examples of the neuro-symbolic paradigm. They use neural networks (LLMs, NLI models, critics) for what they excel at—handling the fuzziness of natural language and pattern recognition—and combine them with symbolic systems (Max-SAT solvers) for what they do best—enforcing logical consistency and performing rigorous, interpretable inference.\n",
        "- **Pioneering Machine Ethics:** The Delphi project is a foundational step in the difficult field of computational ethics. It moves beyond simple toxicity detection to model nuanced moral judgments. While controversial, it opens the door to creating AI systems that are more aligned with human values and can serve as safety filters for other generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Current Research Directions Mentioned\n",
        "\n",
        "The lecture points towards a rich and challenging future research agenda:\n",
        "\n",
        "1.  **Value Pluralism:** The most significant challenge is how to create models that respect diverse cultural, political, and individual values without endorsing harmful ideologies. This is not just an AI problem but one that requires collaboration with humanities, philosophy, and psychology.\n",
        "2.  **Improving Data Quality:** The success of symbolic distillation highlights the need for better data. Future work involves creating more comprehensive, diverse, and less biased knowledge graphs and norm banks, which are essential for training robust models.\n",
        "3.  **Advanced Neuro-Symbolic Architectures:** The Maieutic and Delphi Hybrid models use a pipeline approach. Future research could explore more tightly integrated neuro-symbolic systems where the reasoning process is end-to-end differentiable.\n",
        "4.  **Language Models as Knowledge Models:** The core thesis is that LMs are not knowledge models. A key direction is to explicitly design architectures that build and query an internal, consistent world model, rather than just predicting the next token based on surface-level statistics.\n",
        "5.  **Fact-Checking and Misinformation:** While Delphi deals with norms and ethics, a related and urgent challenge is developing robust models for fact-checking and preventing the spread of misinformation, which requires a similar blend of language understanding and knowledge verification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Applications\n",
        "\n",
        "The research discussed has direct implications for improving the safety and reliability of AI systems deployed in the real world:\n",
        "\n",
        "- **AI Safety Filters:** A model like Delphi, even if imperfect, can act as a powerful safety filter for chatbots, search engines, and smart home devices. It can prevent them from endorsing problematic user statements (e.g., agreeing that \"the Holocaust never happened\") or suggesting dangerous actions (e.g., the home device suggesting a child play with an electrical socket).\n",
        "- **More Reliable Assistants:** By improving logical consistency with methods like Maieutic Prompting, AI assistants can provide more reliable and trustworthy answers to complex questions, reducing the frequency of nonsensical or contradictory outputs.\n",
        "- **Democratizing AI:** Techniques like Symbolic Knowledge Distillation enable the creation of powerful, specialized models that are small enough to run on local hardware, reducing reliance on massive, centralized APIs. This makes advanced AI capabilities more accessible to a wider range of developers and researchers."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
