{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformers in Biomedicine: From Clinical Notes to Genomics\n",
        "\n",
        "## Section 1: Overview & Prerequisites\n",
        "\n",
        "### Summary of the Lecture Topic\n",
        "\n",
        "This notebook explores the transformative impact of Transformers and Large Language Models (LLMs) on the biomedical field, as detailed in the lecture by Dr. Vivek Natarajan. We will journey from the first principles of why sequence-based models like Transformers are a natural fit for biomedical data (clinical notes, electronic health records, proteins, genomes) to specific, state-of-the-art applications. \n",
        "\n",
        "The lecture highlights that while general-purpose LLMs encode a surprising amount of clinical knowledge, they require careful alignment to meet the safety-critical standards of medicine. We will examine **Med-PaLM**, a model aligned using instruction prompt tuning, which closes the gap with expert clinicians in medical question answering. \n",
        "\n",
        "Furthermore, we will delve into the biological stack, exploring architectural innovations required for modeling long biological sequences. This includes the **Performer** architecture for efficient protein modeling, **ProtNLM** for protein function annotation, **DeepConsensus** for improving genomic sequencing accuracy, and **Enformer** for predicting gene expression by modeling long-range DNA interactions. The overarching theme is the convergence of Transformers as a foundational architecture for building powerful, multimodal AI models in biomedicine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite Knowledge\n",
        "\n",
        "**Mathematical Concepts:**\n",
        "- **Linear Algebra:** Vectors, matrices, dot products, matrix multiplication.\n",
        "- **Calculus:** Gradients and their role in optimization (e.g., gradient descent).\n",
        "- **Probability & Statistics:** Softmax function, probability distributions, conditional probability.\n",
        "- **Information Theory:** Cross-entropy loss.\n",
        "- **Matrix Decompositions:** Understanding of low-rank approximation (conceptual level for the Performer model).\n",
        "\n",
        "**Machine Learning / Computer Science Concepts:**\n",
        "- **Fundamentals of Neural Networks:** Neurons, layers, activation functions, backpropagation.\n",
        "- **Sequence Models:** Basic understanding of Recurrent Neural Networks (RNNs) and their limitations (vanishing/exploding gradients).\n",
        "- **The Transformer Architecture:** Self-attention, multi-head attention, positional encodings, encoder-decoder structure.\n",
        "- **Large Language Models (LLMs):** Concepts of pre-training on large corpora and fine-tuning for specific tasks.\n",
        "- **Prompt Engineering:** Few-shot prompting, chain-of-thought prompting.\n",
        "- **Convolutional Neural Networks (CNNs):** Basic understanding of convolutions and receptive fields.\n",
        "- **Biology (Conceptual):**\n",
        "    - **Proteins:** Sequences of amino acids.\n",
        "    - **Genomics:** DNA, base pairs (A, T, C, G), genes, coding vs. non-coding variants, transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchy of Topics & Learning Objectives\n",
        "\n",
        "**Hierarchy:**\n",
        "1.  **Foundations:** Why Transformers are suited for sequential biomedical data.\n",
        "2.  **Core Mechanism:** Deep dive into the self-attention mechanism, the engine of Transformers.\n",
        "3.  **Clinical Applications:** Applying LLMs to medical question answering (Med-PaLM) and the importance of domain alignment.\n",
        "4.  **Biological Applications & Architectural Innovations:**\n",
        "    - Modeling long protein sequences efficiently (Performer).\n",
        "    - Annotating protein functions (ProtNLM).\n",
        "    - Correcting DNA sequencing errors (DeepConsensus).\n",
        "    - Predicting gene expression from DNA (Enformer).\n",
        "5.  **Experimental Analysis & Future Directions:** Reproducing key experimental insights and discussing the future of biomedical AI.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand the fundamental reasons for applying Transformers to biomedical data.\n",
        "- Implement the core self-attention mechanism from scratch.\n",
        "- Grasp the concept and importance of aligning LLMs for safety-critical domains like medicine.\n",
        "- Understand the architectural challenges posed by long biological sequences and the solutions proposed (e.g., Performer, Enformer).\n",
        "- Appreciate the breadth of applications for Transformers in biomedicine, from clinical practice to fundamental biology.\n",
        "\n",
        "**Estimated Time:** 2 - 3 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2: Mathematical Foundations\n",
        "\n",
        "The core innovation of the Transformer is the **self-attention mechanism**. It allows the model to weigh the importance of different words (or tokens, amino acids, nucleotides) in an input sequence when processing a specific word. This directly addresses the challenge of modeling long-range dependencies, a key feature in all the biomedical applications discussed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import time\n",
        "import math\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True)) # Subtract max for numerical stability\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def educational_self_attention(X, mask=None):\n",
        "    \"\"\"\n",
        "    Clear implementation of self-attention for understanding.\n",
        "    - Based directly on the original Transformer paper.\n",
        "    - Extensive comments explaining each step.\n",
        "    - No black-box libraries for the core concept.\n",
        "    \n",
        "    Args:\n",
        "        X (np.array): Input matrix of shape (sequence_length, embedding_dim).\n",
        "        mask (np.array): Optional mask to hide certain positions.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[np.array, np.array]: The context vectors (output) and the attention weights.\n",
        "    \"\"\"\n",
        "    seq_len, d_model = X.shape\n",
        "    d_k = d_v = d_model # For simplicity, let key/value/query dims be the same\n",
        "\n",
        "    # Step 1: Initialize weight matrices for Query, Key, Value.\n",
        "    # In a real model, these are learned during training.\n",
        "    # Here, we use random matrices for demonstration.\n",
        "    np.random.seed(42)\n",
        "    W_q = np.random.randn(d_model, d_k)\n",
        "    W_k = np.random.randn(d_model, d_k)\n",
        "    W_v = np.random.randn(d_model, d_v)\n",
        "\n",
        "    # Step 2: Project the input X into Query, Key, and Value spaces.\n",
        "    # Q, K, V are the core components of attention.\n",
        "    Q = X @ W_q\n",
        "    K = X @ W_k\n",
        "    V = X @ W_v\n",
        "\n",
        "    # Step 3: Calculate attention scores.\n",
        "    # This is the dot product of a query with all keys. It measures similarity.\n",
        "    # The result is a (seq_len, seq_len) matrix where scores[i, j] is the attention\n",
        "    # of token i to token j.\n",
        "    scores = (Q @ K.T) / np.sqrt(d_k)\n",
        "\n",
        "    # Step 4: Apply mask (if provided).\n",
        "    # This is used in decoders to prevent a position from attending to future positions.\n",
        "    if mask is not None:\n",
        "        scores = np.where(mask == 0, -1e9, scores) # Set masked values to a large negative number\n",
        "\n",
        "    # Step 5: Convert scores to probabilities (attention weights) using softmax.\n",
        "    # The weights for each token will sum to 1.\n",
        "    attention_weights = softmax(scores)\n",
        "\n",
        "    # Step 6: Compute the context vector (the output of the attention layer).\n",
        "    # This is a weighted sum of the Value vectors, where the weights are the\n",
        "    # attention probabilities. Tokens with higher attention contribute more.\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    return context_vectors, attention_weights\n",
        "\n",
        "# --- Demonstration ---\n",
        "# Let's simulate a sequence of 5 tokens (e.g., amino acids), each with an embedding of size 8.\n",
        "sequence_length = 5\n",
        "embedding_dim = 8\n",
        "X_input = np.random.randn(sequence_length, embedding_dim)\n",
        "\n",
        "context, weights = educational_self_attention(X_input)\n",
        "\n",
        "print(\"Input Shape:\", X_input.shape)\n",
        "print(\"Output (Context Vectors) Shape:\", context.shape)\n",
        "print(\"Attention Weights Shape:\", weights.shape)\n",
        "print(\"\\nAttention Weights (rounded):\")\n",
        "print(np.round(weights, 2))\n",
        "\n",
        "# Verify that weights for each token sum to 1\n",
        "print(\"\\nSum of weights for each token:\", np.sum(weights, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Visualization of Self-Attention\n",
        "\n",
        "The matrix of attention weights shows *which* tokens are paying attention to *which other* tokens. A high value at `weights[i, j]` means that when the model is processing token `i`, it places a high importance on token `j`.\n",
        "\n",
        "Use the slider below to select a token from the input sequence and visualize its attention weights with respect to all other tokens in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_attention_explorer():\n",
        "    \"\"\"\n",
        "    Interactive widget to explore attention weights.\n",
        "    \"\"\"\n",
        "    # Use the weights calculated from the previous cell\n",
        "    labels = [f\"Token {i+1}\" for i in range(sequence_length)]\n",
        "\n",
        "    def plot_attention(token_idx):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot the full attention matrix\n",
        "        ax1 = plt.subplot(1, 2, 1)\n",
        "        sns.heatmap(weights, annot=True, fmt=\".2f\", cmap='viridis', xticklabels=labels, yticklabels=labels, ax=ax1)\n",
        "        ax1.set_title('Full Attention Matrix (W)')\n",
        "        ax1.set_xlabel('Key Tokens')\n",
        "        ax1.set_ylabel('Query Tokens')\n",
        "        \n",
        "        # Highlight the selected query token's row\n",
        "        rect = plt.Rectangle((0, token_idx), sequence_length, 1, fill=False, edgecolor='red', lw=3)\n",
        "        ax1.add_patch(rect)\n",
        "\n",
        "        # Plot the attention weights for the selected token\n",
        "        ax2 = plt.subplot(1, 2, 2)\n",
        "        ax2.bar(labels, weights[token_idx])\n",
        "        ax2.set_title(f'Attention from {labels[token_idx]} to others')\n",
        "        ax2.set_ylabel('Attention Weight')\n",
        "        ax2.set_ylim(0, 1.0)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    token_slider = widgets.IntSlider(\n",
        "        value=0, min=0, max=sequence_length - 1, step=1, description='Query Token:'\n",
        "    )\n",
        "    \n",
        "    widgets.interactive(plot_attention, token_idx=token_slider)\n",
        "\n",
        "interactive_attention_explorer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3: Prerequisite Algorithms\n",
        "\n",
        "### Building a Transformer Block\n",
        "\n",
        "Before tackling the advanced models from the lecture, it's crucial to understand a standard Transformer block. This block is the fundamental repeating unit in Transformer architectures. It typically consists of:\n",
        "\n",
        "1.  **Multi-Head Self-Attention:** An extension of the self-attention we just implemented. It runs the attention mechanism multiple times in parallel with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
        "2.  **Add & Norm (Layer Normalization):** A residual connection (adding the input to the output of the attention layer) followed by layer normalization. This helps stabilize training.\n",
        "3.  **Feed-Forward Network:** A simple position-wise fully connected neural network.\n",
        "4.  **Another Add & Norm Layer.**\n",
        "\n",
        "Let's build a conceptual implementation to see how these pieces fit together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EducationalTransformerBlock:\n",
        "    \"\"\"\n",
        "    A simplified, educational implementation of a Transformer block.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input embeddings.\n",
        "            num_heads (int): The number of attention heads.\n",
        "        \"\"\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        # --- Multi-Head Attention Parameters ---\n",
        "        # In a real model, these are learned. We initialize them randomly.\n",
        "        self.W_q = [np.random.randn(d_model, self.d_head) for _ in range(num_heads)]\n",
        "        self.W_k = [np.random.randn(d_model, self.d_head) for _ in range(num_heads)]\n",
        "        self.W_v = [np.random.randn(d_model, self.d_head) for _ in range(num_heads)]\n",
        "        self.W_o = np.random.randn(d_model, d_model) # Output projection matrix\n",
        "\n",
        "        # --- Feed-Forward Network Parameters ---\n",
        "        self.W1_ff = np.random.randn(d_model, d_model * 4)\n",
        "        self.b1_ff = np.random.randn(d_model * 4)\n",
        "        self.W2_ff = np.random.randn(d_model * 4, d_model)\n",
        "        self.b2_ff = np.random.randn(d_model)\n",
        "    \n",
        "    def _layer_norm(self, x, epsilon=1e-5):\n",
        "        # Simple from-scratch layer normalization\n",
        "        mean = x.mean(axis=-1, keepdims=True)\n",
        "        std = x.std(axis=-1, keepdims=True)\n",
        "        return (x - mean) / (std + epsilon)\n",
        "\n",
        "    def _multi_head_attention(self, X):\n",
        "        heads = []\n",
        "        for i in range(self.num_heads):\n",
        "            # Project to Q, K, V for this head\n",
        "            Q_i = X @ self.W_q[i]\n",
        "            K_i = X @ self.W_k[i]\n",
        "            V_i = X @ self.W_v[i]\n",
        "            \n",
        "            # Calculate attention scores for this head\n",
        "            scores_i = (Q_i @ K_i.T) / np.sqrt(self.d_head)\n",
        "            weights_i = softmax(scores_i)\n",
        "            \n",
        "            # Calculate context for this head\n",
        "            head_output = weights_i @ V_i\n",
        "            heads.append(head_output)\n",
        "        \n",
        "        # Concatenate heads and apply final linear projection\n",
        "        concatenated_heads = np.concatenate(heads, axis=-1)\n",
        "        mha_output = concatenated_heads @ self.W_o\n",
        "        return mha_output\n",
        "\n",
        "    def _feed_forward(self, x):\n",
        "        # Simple two-layer feed-forward network with ReLU activation\n",
        "        hidden = np.maximum(0, x @ self.W1_ff + self.b1_ff) # ReLU\n",
        "        output = hidden @ self.W2_ff + self.b2_ff\n",
        "        return output\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass through the Transformer block.\"\"\"\n",
        "        # 1. Multi-Head Attention sub-layer\n",
        "        attention_output = self._multi_head_attention(X)\n",
        "        \n",
        "        # 2. Add & Norm for the attention sub-layer\n",
        "        sublayer1_output = self._layer_norm(X + attention_output)\n",
        "        \n",
        "        # 3. Feed-Forward sub-layer\n",
        "        ff_output = self._feed_forward(sublayer1_output)\n",
        "        \n",
        "        # 4. Add & Norm for the feed-forward sub-layer\n",
        "        block_output = self._layer_norm(sublayer1_output + ff_output)\n",
        "        \n",
        "        return block_output\n",
        "\n",
        "# --- Demonstration ---\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "input_sequence = np.random.randn(seq_len, d_model)\n",
        "\n",
        "transformer_block = EducationalTransformerBlock(d_model=d_model, num_heads=num_heads)\n",
        "output_sequence = transformer_block.forward(input_sequence)\n",
        "\n",
        "print(\"Input Sequence Shape:\", input_sequence.shape)\n",
        "print(\"Output Sequence Shape:\", output_sequence.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4: Core Research Content from the Lecture\n",
        "\n",
        "Now we will dive into the specific models and concepts presented in the lecture, building upon our foundational understanding of Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Med-PaLM: Aligning LLMs for Clinical Applications\n",
        "\n",
        "**Motivation:** General-purpose LLMs like FLAN-PaLM, while knowledgeable, are not ready for clinical use out-of-the-box. They can hallucinate, provide incomplete answers, and fail to meet the high safety and quality bar of the medical domain. \n",
        "\n",
        "**Contribution:** The Med-PaLM paper introduces a methodology for aligning an LLM to the medical domain using a highly data- and compute-efficient technique called **Instruction Prompt Tuning**.\n",
        "\n",
        "**Instruction Prompt Tuning:**\n",
        "- The massive base LLM (e.g., 540B parameter FLAN-PaLM) is **frozen**. Its weights are not updated.\n",
        "- A small set of new, trainable vectors (a \"soft prompt\") are prepended to the input.\n",
        "- During tuning, only these prompt vectors are updated using a small, high-quality dataset of instruction-response pairs curated by clinicians.\n",
        "- This teaches the model *how* to access and formulate its existing medical knowledge in a way that aligns with clinical expectations (e.g., being comprehensive, cautious, and avoiding harm).\n",
        "\n",
        "Below is a conceptual code block illustrating how one might set up such a tuning process. Note that we cannot run this as we don't have access to the PaLM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conceptual_instruction_prompt_tuning():\n",
        "    \"\"\"\n",
        "    Conceptual demonstration of instruction prompt tuning.\n",
        "    This is purely illustrative.\n",
        "    \"\"\"\n",
        "    \n",
        "    class MockLargeLanguageModel:\n",
        "        def __init__(self):\n",
        "            print(\"Initialized a massive, FROZEN language model (e.g., FLAN-PaLM 540B).\")\n",
        "            # In reality, this would contain billions of learned weights.\n",
        "            self.is_frozen = True\n",
        "        \n",
        "        def forward(self, input_embedding):\n",
        "            # This is a massive simplification of the forward pass.\n",
        "            print(f\"LLM processing input of shape: {input_embedding.shape}\")\n",
        "            # The model generates text based on the input embedding.\n",
        "            return \"...generated medical answer...\"\n",
        "\n",
        "    # 1. Load the massive, pre-trained base model and freeze it.\n",
        "    base_llm = MockLargeLanguageModel()\n",
        "\n",
        "    # 2. Define a small, trainable prompt.\n",
        "    # Let's say our prompt consists of 20 tokens, and model embedding size is 1024.\n",
        "    prompt_length = 20\n",
        "    embedding_dim = 1024\n",
        "    trainable_prompt_vectors = np.random.randn(prompt_length, embedding_dim) # These are the ONLY parameters we train!\n",
        "\n",
        "    # 3. Prepare the actual input.\n",
        "    # Example question from the MultiMedQA benchmark.\n",
        "    input_question = \"What are the side effects of metformin?\"\n",
        "    # This text would be converted to token embeddings.\n",
        "    # Let's say it becomes 8 tokens.\n",
        "    input_embeddings = np.random.randn(8, embedding_dim)\n",
        "\n",
        "    # 4. Prepend the trainable prompt to the input embeddings.\n",
        "    combined_input = np.concatenate([trainable_prompt_vectors, input_embeddings], axis=0)\n",
        "    print(f\"Original input shape: {input_embeddings.shape}\")\n",
        "    print(f\"Trainable prompt shape: {trainable_prompt_vectors.shape}\")\n",
        "    print(f\"Combined input shape for LLM: {combined_input.shape}\")\n",
        "\n",
        "    # 5. Get the model's output.\n",
        "    model_output = base_llm.forward(combined_input)\n",
        "\n",
        "    # 6. During training:\n",
        "    #    - Compare `model_output` with a clinician-curated reference answer.\n",
        "    #    - Calculate a loss.\n",
        "    #    - Backpropagate the error to update ONLY `trainable_prompt_vectors`.\n",
        "    #    - The `base_llm` weights remain unchanged.\n",
        "    print(\"\\n--- Training Phase (Conceptual) ---\")\n",
        "    print(\"Model output compared to clinician's ideal answer.\")\n",
        "    print(\"Loss is computed and gradients are backpropagated.\")\n",
        "    print(\"ONLY the trainable_prompt_vectors are updated.\")\n",
        "    print(\"The massive base_llm remains frozen. This is highly efficient!\")\n",
        "\n",
        "conceptual_instruction_prompt_tuning()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Performer: Efficient Transformers for Long Biological Sequences\n",
        "\n",
        "**Motivation:** The standard self-attention mechanism has a complexity of O(L²) with respect to the sequence length L, both in time and memory. This is because it computes a full L x L attention matrix. For long biological sequences like proteins (thousands of amino acids) or DNA (billions of base pairs), this is computationally prohibitive.\n",
        "\n",
        "**Contribution:** The Performer model introduces an efficient Transformer architecture that approximates the softmax attention mechanism, reducing the complexity to O(L). It does this via a technique called **FAVOR+ (Fast Attention Via Positive Orthogonal Random Features)**.\n",
        "\n",
        "**Mathematical Derivation (High-Level):**\n",
        "1.  The standard attention is calculated as `Attention(Q, K, V) = softmax(QKᵀ/√d_k)V`.\n",
        "2.  The `softmax` function can be expressed using `exp`. The core computation is `exp(q_i • k_j)` for each query `q_i` and key `k_j`.\n",
        "3.  The key insight is that the exponential kernel `exp(x•y)` can be approximated by the dot product of high-dimensional random feature maps: `exp(x•y) ≈ E[φ(x) • φ(y)]`, where `φ` is a mapping to a higher dimensional space using random features.\n",
        "4.  By applying this, the attention matrix `A = exp(QKᵀ)` can be approximated as `A ≈ Q'K'ᵀ`, where `Q'` and `K'` are the inputs mapped by `φ`.\n",
        "5.  The final computation becomes `(Q' (K'ᵀ V))`. Crucially, the order of multiplication is changed. Instead of computing the `L x L` matrix `Q'K'ᵀ`, we first compute the `m x d_v` matrix `K'ᵀ V` (where `m` is the number of random features, and `m << L`). This avoids the quadratic bottleneck.\n",
        "\n",
        "The result is a linear-time and linear-memory attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_performer_attention(Q, K, V, num_random_features):\n",
        "    \"\"\"\n",
        "    Clear implementation of the Performer's core approximation for understanding.\n",
        "    - Based directly on the FAVOR+ mechanism described in the paper.\n",
        "    - No black-box libraries for the core approximation.\n",
        "    \n",
        "    Args:\n",
        "        Q, K, V: Query, Key, Value matrices of shape (seq_len, d_k).\n",
        "        num_random_features (int): The number of random features (m) to use for approximation.\n",
        "    \n",
        "    Returns:\n",
        "        np.array: The approximated context vectors.\n",
        "    \"\"\"\n",
        "    seq_len, d_k = Q.shape\n",
        "\n",
        "    # Step 1: Generate a random projection matrix.\n",
        "    # This is the core of the random feature map φ.\n",
        "    np.random.seed(0)\n",
        "    random_projection_matrix = np.random.randn(d_k, num_random_features)\n",
        "\n",
        "    # Step 2: Define the feature map φ(x) = exp(xW) / sqrt(m).\n",
        "    # The paper uses a more complex positive orthogonal feature map for better stability,\n",
        "    # but this captures the main idea.\n",
        "    def feature_map(x):\n",
        "        # Project the input\n",
        "        projected_x = x @ random_projection_matrix\n",
        "        # Apply the kernel approximation components\n",
        "        return np.exp(projected_x) / np.sqrt(num_random_features)\n",
        "    \n",
        "    # Step 3: Map Q and K to the random feature space.\n",
        "    Q_prime = feature_map(Q)\n",
        "    K_prime = feature_map(K)\n",
        "\n",
        "    # Step 4: Compute attention by re-ordering matrix multiplications.\n",
        "    # This is the key step that avoids the quadratic L x L matrix.\n",
        "    \n",
        "    # First, compute D_inv, a normalization factor (denominator of softmax).\n",
        "    # This is equivalent to summing the rows of the approximated attention matrix.\n",
        "    D_inv = 1.0 / (Q_prime @ K_prime.sum(axis=0))\n",
        "    D_inv = np.diag(D_inv)\n",
        "    \n",
        "    # Next, compute K_prime.T @ V. Shape: (m, d_v)\n",
        "    # This aggregates values based on their key projections.\n",
        "    K_prime_T_V = K_prime.T @ V\n",
        "\n",
        "    # Now, compute Q_prime @ (K_prime.T @ V). Shape: (L, d_v)\n",
        "    # This calculates the weighted sum for each query.\n",
        "    Q_prime_K_prime_T_V = Q_prime @ K_prime_T_V\n",
        "\n",
        "    # Finally, apply the normalization.\n",
        "    approximated_context = D_inv @ Q_prime_K_prime_T_V\n",
        "    \n",
        "    return approximated_context\n",
        "\n",
        "\n",
        "# --- Demonstration ---\n",
        "L = 1024  # A longer sequence length\n",
        "d = 64\n",
        "m = 128   # Number of random features (m << L)\n",
        "\n",
        "Q_long = np.random.randn(L, d)\n",
        "K_long = np.random.randn(L, d)\n",
        "V_long = np.random.randn(L, d)\n",
        "\n",
        "print(f\"Sequence Length (L): {L}\")\n",
        "print(f\"Embedding Dim (d): {d}\")\n",
        "print(f\"Num Random Features (m): {m}\")\n",
        "\n",
        "context_performer = educational_performer_attention(Q_long, K_long, V_long, num_random_features=m)\n",
        "\n",
        "print(\"\\nPerformer Approximated Output Shape:\", context_performer.shape)\n",
        "print(\"Notice that no (L x L) matrix was created in the process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 ProtNLM: Annotating Proteins with Natural Language\n",
        "\n",
        "**Motivation:** Over 50% of known protein sequences are uncharacterized, meaning their function is unknown. Manually annotating them is a massive bottleneck. Automating this process could significantly accelerate biological research.\n",
        "\n",
        "**Contribution:** ProtNLM frames this as a sequence-to-sequence task, analogous to machine translation or image captioning. It uses a T5 (Text-to-Text Transfer Transformer) model to \"translate\" a sequence of amino acids into a natural language description of the protein's function.\n",
        "\n",
        "- **Input:** A sequence of amino acids (e.g., `M A D Q G V...`)\n",
        "- **Output:** A natural language text (e.g., `\"This protein is involved in ATP binding and metabolic processes.\"`) \n",
        "\n",
        "The model was trained on the UniProt database, which contains existing protein sequences and their annotations. This approach successfully generated plausible descriptions for 49 million previously uncharacterized proteins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 DeepConsensus: Improving Genome Sequencing Accuracy\n",
        "\n",
        "**Motivation:** Modern DNA sequencers (like PacBio) read a DNA molecule multiple times to generate several \"subreads\". These are then combined into a single Circular Consensus Sequence (CCS). However, this process still contains errors.\n",
        "\n",
        "**Contribution:** DeepConsensus is a Transformer-based model that takes the raw subreads and the initial CCS read as input to produce a more accurate, polished sequence. \n",
        "\n",
        "- **Input:** Multiple noisy subreads of a DNA segment and other instrument features.\n",
        "- **Output:** A single, corrected DNA sequence.\n",
        "\n",
        "A key innovation mentioned in the lecture is the use of a special **alignment-based loss function** instead of standard cross-entropy loss. \n",
        "\n",
        "**Why not Cross-Entropy?**\n",
        "DNA sequences can have insertion and deletion errors. If the model predicts a sequence that is shifted by one base, cross-entropy loss would penalize every single subsequent prediction, even if the rest of the sequence is correct. An alignment loss (related to edit distance) can correctly identify that the error is a single insertion/deletion and provide a more meaningful gradient for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Enformer: Predicting Gene Expression from Long-Range DNA Interactions\n",
        "\n",
        "**Motivation:** 90% of genetic variants associated with diseases lie in non-coding regions of DNA. These regions don't code for proteins but act as regulatory elements (like \"enhancers\"). An enhancer can be very far from a gene in the linear DNA sequence but influence its expression because the DNA folds in 3D space, bringing them close together. Predicting these long-range effects is a major challenge.\n",
        "\n",
        "**Contribution:** The Enformer model uses a hybrid architecture (CNNs followed by Transformer blocks) to predict gene expression from DNA sequences. The Transformer's ability to model long-range dependencies is critical here.\n",
        "\n",
        "- **Input:** A long DNA sequence (200k base pairs).\n",
        "- **Output:** A prediction of gene expression levels across thousands of genomic tracks.\n",
        "- **Architecture:** The initial CNN layers efficiently learn local patterns, and their output is then fed into Transformer blocks which can model interactions between elements that are hundreds of thousands of base pairs apart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5: Experimental Analysis\n",
        "\n",
        "This section reproduces some of the key experimental findings and comparisons discussed in the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Med-PaLM: Closing the Gap to Clinicians\n",
        "\n",
        "The lecture presented results from a comprehensive human evaluation framework for the medical question answering task. Long-form answers from **FLAN-PaLM** (the base model), **Med-PaLM** (the aligned model), and expert **Clinicians** were rated by other clinicians and lay users across several axes.\n",
        "\n",
        "**Key Findings:**\n",
        "- **FLAN-PaLM vs. Med-PaLM:** The instruction-prompt-tuned Med-PaLM consistently and significantly outperformed the general-purpose FLAN-PaLM across axes like factual accuracy, medical reasoning, and potential for harm.\n",
        "- **Med-PaLM vs. Clinicians:** While clinicians still performed best overall, Med-PaLM closed the gap considerably. For example, in terms of answers agreeing with scientific consensus, FLAN-PaLM was rated at ~60%, while Med-PaLM jumped to over 90%, very close to the clinician-generated answers.\n",
        "- **Lay User Helpfulness:** For non-expert users, Med-PaLM's answers were rated as helpful 80% of the time, a large improvement over FLAN-PaLM's 60%, but still below the clinicians' 90%. This highlights the importance of evaluating from multiple perspectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Performer: Parameter Sensitivity and Efficiency Gains\n",
        "\n",
        "A key claim of the Performer paper is its linear complexity. Let's analyze the computational complexity of standard attention versus the Performer's approximation. The dominant operation in standard attention is the `Q @ K.T` matrix multiplication, which is O(L²d). In Performer, the dominant steps are the random feature projections `Q @ W` (O(Ldm)) and the final aggregations like `Q_prime @ K_prime_T_V` (O(Lmd)), resulting in an overall complexity of O(Lmd). Since `m` is a fixed hyperparameter independent of `L`, this is **O(L)**.\n",
        "\n",
        "Let's visualize this scaling difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {}
    }
  ]
}
