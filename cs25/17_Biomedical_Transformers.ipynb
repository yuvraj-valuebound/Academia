{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformers in Biomedicine: From Clinical Language to Genomics\n",
        "\n",
        "**An Interactive Notebook Based on the Lecture by Vivek Natarajan, Google Health AI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Overview & Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of the Lecture Topic\n",
        "\n",
        "This notebook explores the transformative impact of Transformer architectures and Large Language Models (LLMs) on the field of biomedicine, based on a lecture by Vivek Natarajan. The core thesis is that sequences are a ubiquitous data modality in biomedicine—from clinical notes and electronic health records to proteins (sequences of amino acids) and genomes (sequences of nucleotides). Transformers, with their ability to model complex, long-range dependencies, represent a powerful tool for this domain.\n",
        "\n",
        "We will delve into several key research papers discussed in the lecture, covering a spectrum of applications:\n",
        "1.  **Clinical Applications:** How LLMs like Med-PaLM are being aligned for medical question answering, requiring innovations in benchmarks (`MultiMedQA`), evaluation frameworks, and alignment techniques (Instruction Prompt Tuning).\n",
        "2.  **Proteomics:** How efficient Transformer architectures like the Performer can model long protein sequences, and how models like ProtNLM can annotate protein functions at a massive scale.\n",
        "3.  **Genomics:** How Transformers are used in `DeepConsensus` to improve the accuracy of DNA sequencing and in `Enformer` to predict gene expression by modeling long-range interactions in the genome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite Knowledge\n",
        "\n",
        "#### Mathematical Concepts\n",
        "- **Linear Algebra:** Vector and matrix operations, particularly the dot product, which is the foundation of self-attention.\n",
        "- **Probability & Statistics:** Understanding of the Softmax function for converting scores into a probability distribution.\n",
        "- **Calculus:** Basic understanding of gradients for model training (conceptual).\n",
        "- **Low-Rank Matrix Approximation:** Conceptual understanding for the Performer model.\n",
        "\n",
        "#### Machine Learning & Computer Science Concepts\n",
        "- **Neural Networks:** Fundamentals of deep learning and backpropagation.\n",
        "- **Sequence Models:** Basic knowledge of why models like RNNs struggle with long sequences (vanishing gradients).\n",
        "- **The Transformer Architecture:** Self-Attention, Multi-Head Attention, Positional Encodings, Encoder-Decoder structure.\n",
        "- **Large Language Models (LLMs):** The pre-training and fine-tuning paradigm.\n",
        "- **Prompting Techniques:** Few-shot prompting, Chain of Thought, Self-Consistency (conceptual).\n",
        "- **Parameter-Efficient Fine-Tuning (PEFT):** Specifically, the concept of Prompt Tuning.\n",
        "- **Convolutional Neural Networks (CNNs):** Understanding their use in sequence modeling and the concept of a receptive field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchy of Topics\n",
        "\n",
        "1.  **Mathematical Foundations:** We'll start by implementing the core mechanism of Transformers—the self-attention mechanism—from scratch.\n",
        "2.  **Prerequisite Algorithms:** We'll build a basic Transformer block and review CNNs for sequence data to set the stage for later models.\n",
        "3.  **Core Research: Clinical Applications (Med-PaLM):** We will explore Instruction Prompt Tuning, the technique used to align a general LLM to the medical domain.\n",
        "4.  **Core Research: Proteomics (Performer & ProtNLM):** We'll implement the core idea behind the Performer's efficient attention and discuss the T5-based approach of ProtNLM.\n",
        "5.  **Core Research: Genomics (Enformer):** We'll build a simplified model to demonstrate the power of Transformers over CNNs for capturing long-range genomic interactions.\n",
        "6.  **Experimental Analysis:** We will reproduce toy versions of the experiments discussed, such as evaluating model answers and comparing the performance of CNNs vs. Transformers on synthetic genomic data.\n",
        "7.  **Research Context & Extensions:** We'll conclude by summarizing the key takeaways and future directions outlined in the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Objectives\n",
        "- **Understand** why Transformers are exceptionally well-suited for diverse biomedical data.\n",
        "- **Implement** the self-attention mechanism from scratch to grasp its inner workings.\n",
        "- **Grasp** the concept of Instruction Prompt Tuning for domain-specific model alignment.\n",
        "- **Appreciate** the architectural innovations required to apply Transformers to long sequences in genomics and proteomics.\n",
        "- **Reproduce** the core experimental findings that demonstrate the superiority of these models in their respective domains.\n",
        "\n",
        "**Estimated Time:** 2-3 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Mathematical Foundations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Heart of the Transformer: Self-Attention\n",
        "\n",
        "The core innovation of the Transformer is the self-attention mechanism. It allows the model to weigh the importance of different words (or tokens, amino acids, nucleotides) in the input sequence when processing a specific word. It computes a representation of a token by relating it to all other tokens in the sequence.\n",
        "\n",
        "The formula is:\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
        "\n",
        "Where:\n",
        "- $Q$ (Query), $K$ (Key), and $V$ (Value) are matrices derived from the input embeddings.\n",
        "- $d_k$ is the dimension of the key vectors, used for scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def educational_self_attention(x, d_model, d_k):\n",
        "    \"\"\"\n",
        "    Clear implementation of self-attention for understanding.\n",
        "    - Based directly on the mathematical formula.\n",
        "    - Extensive comments explaining each step.\n",
        "    - No black-box libraries for the core logic.\n",
        "    \n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (seq_len, d_model).\n",
        "        d_model (int): The embedding dimension.\n",
        "        d_k (int): The dimension for Key/Query vectors.\n",
        "    \"\"\"\n",
        "    seq_len = x.shape[0]\n",
        "    \n",
        "    # 1. Create linear projections for Query, Key, and Value\n",
        "    # In a real model, these would be learned weight matrices (nn.Linear)\n",
        "    W_q = torch.randn(d_model, d_k)\n",
        "    W_k = torch.randn(d_model, d_k)\n",
        "    W_v = torch.randn(d_model, d_k) # d_v is often same as d_k\n",
        "    \n",
        "    # 2. Project the input into Query, Key, and Value spaces\n",
        "    Q = x @ W_q  # (seq_len, d_k)\n",
        "    K = x @ W_k  # (seq_len, d_k)\n",
        "    V = x @ W_v  # (seq_len, d_k)\n",
        "    \n",
        "    # 3. Calculate attention scores: Q * K^T\n",
        "    # This measures the similarity between each query and all keys.\n",
        "    scores = Q @ K.T  # (seq_len, seq_len)\n",
        "    \n",
        "    # 4. Scale the scores to stabilize gradients\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "    \n",
        "    # 5. Apply softmax to get attention weights (probabilities)\n",
        "    # This makes the weights for each token sum to 1.\n",
        "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
        "    \n",
        "    # 6. Apply attention weights to the Value vectors\n",
        "    # This creates a weighted sum of values, where the weights are the attention scores.\n",
        "    output = attention_weights @ V  # (seq_len, d_k)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# --- Example Usage ---\n",
        "seq_len = 5 # e.g., 5 amino acids in a protein sequence\n",
        "d_model = 32 # embedding dimension for each amino acid\n",
        "d_k = 16 # dimension of Q, K, V projections\n",
        "\n",
        "# Create a dummy input sequence (e.g., embeddings for \"A C G T C\")\n",
        "input_sequence = torch.randn(seq_len, d_model)\n",
        "\n",
        "output, weights = educational_self_attention(input_sequence, d_model, d_k)\n",
        "\n",
        "print(\"Input Shape:\", input_sequence.shape)\n",
        "print(\"Output Shape:\", output.shape)\n",
        "print(\"Attention Weights Shape:\", weights.shape)\n",
        "\n",
        "def interactive_attention_explorer():\n",
        "    \"\"\"\n",
        "    Interactive widget to visualize attention weights.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(weights.detach().numpy(), annot=True, cmap='viridis', xticklabels=range(seq_len), yticklabels=range(seq_len))\n",
        "    plt.title(\"Self-Attention Weights\")\n",
        "    plt.xlabel(\"Key Positions\")\n",
        "    plt.ylabel(\"Query Positions\")\n",
        "    plt.show()\n",
        "\n",
        "interactive_attention_explorer()\n",
        "print(\"\\nEach row shows how much the model 'attends' to every other token when processing the token at that row's index.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Prerequisite Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite 1: A Basic Transformer Block\n",
        "\n",
        "A full Transformer is built by stacking several blocks. A single block typically contains:\n",
        "1.  **Multi-Head Attention:** An evolution of self-attention where the attention mechanism is run multiple times in parallel with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces.\n",
        "2.  **Add & Norm (Layer Normalization):** A residual connection followed by layer normalization, which helps stabilize training.\n",
        "3.  **Feed-Forward Network:** A simple position-wise fully connected network applied to each position separately and identically.\n",
        "4.  **Add & Norm:** Another residual connection and layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EducationalTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Clear implementation of a Transformer block for understanding.\n",
        "    - Based directly on the \"Attention is All You Need\" paper.\n",
        "    - Uses PyTorch's optimized layers but shows how they connect.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EducationalTransformerBlock, self).__init__()\n",
        "        \n",
        "        # Multi-Head Attention Layer\n",
        "        self.multi_head_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "        \n",
        "        # Feed-Forward Network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        \n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # 1. Multi-Head Attention followed by Add & Norm\n",
        "        attn_output, _ = self.multi_head_attn(src, src, src) # Q, K, V are all the same 'src'\n",
        "        src = src + self.dropout1(attn_output) # Residual connection\n",
        "        src = self.norm1(src) # Layer Normalization\n",
        "        \n",
        "        # 2. Feed-Forward Network followed by Add & Norm\n",
        "        ffn_output = self.ffn(src)\n",
        "        src = src + self.dropout2(ffn_output) # Residual connection\n",
        "        src = self.norm2(src) # Layer Normalization\n",
        "        \n",
        "        return src\n",
        "\n",
        "# --- Example Usage ---\n",
        "batch_size = 1\n",
        "seq_len = 10 # 10 tokens\n",
        "d_model = 512 # Model dimension\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of the feed-forward layer\n",
        "\n",
        "transformer_block = EducationalTransformerBlock(d_model, num_heads, d_ff)\n",
        "\n",
        "# Input needs to be (batch_size, seq_len, d_model)\n",
        "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
        "output = transformer_block(dummy_input)\n",
        "\n",
        "print(\"Input Shape:\", dummy_input.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite 2: CNNs for Sequence Data\n",
        "\n",
        "Before Transformers, CNNs were often used for sequence tasks. They use kernels (filters) that slide across the sequence, capturing local patterns. Their primary limitation, especially relevant for the `Enformer` paper, is their **fixed and local receptive field**. A deep stack of CNN layers is required to see tokens that are far apart, making it difficult to model long-range dependencies efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_receptive_field():\n",
        "    \"\"\"Visualize the limited receptive field of a CNN vs a Transformer.\"\"\"\n",
        "    \n",
        "    # A CNN with a kernel size of 3 only sees its immediate neighbors.\n",
        "    cnn_receptive_field = np.zeros((10, 10))\n",
        "    for i in range(10):\n",
        "        cnn_receptive_field[i, max(0, i-1):min(10, i+2)] = 1\n",
        "        \n",
        "    # A Transformer's self-attention can see every other token from the start.\n",
        "    transformer_receptive_field = np.ones((10, 10))\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    sns.heatmap(cnn_receptive_field, ax=ax1, cbar=False, cmap='Reds', linewidths=.5)\n",
        "    ax1.set_title(\"CNN Receptive Field (1 Layer, Kernel=3)\")\n",
        "    ax1.set_xlabel(\"Input Sequence\")\n",
        "    ax1.set_ylabel(\"Output Neuron\")\n",
        "    \n",
        "    sns.heatmap(transformer_receptive_field, ax=ax2, cbar=False, cmap='Blues', linewidths=.5)\n",
        "    ax2.set_title(\"Transformer Receptive Field (1 Layer)\")\n",
        "    ax2.set_xlabel(\"Input Sequence\")\n",
        "    ax2.set_ylabel(\"Output Neuron\")\n",
        "    \n",
        "    plt.suptitle(\"Receptive Field Comparison\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_receptive_field()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Core Research Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Med-PaLM: Aligning LLMs with Instruction Prompt Tuning\n",
        "\n",
        "A key challenge highlighted in the lecture was that general-purpose LLMs like FLAN-PaLM, despite encoding significant medical knowledge, are not directly usable in clinical settings due to issues like hallucination and a lack of caution. The solution presented was **Instruction Prompt Tuning**, a parameter-efficient method.\n",
        "\n",
        "**How it works:**\n",
        "1.  The large base LLM (e.g., FLAN-PaLM) is **frozen**. Its billions of parameters are not updated.\n",
        "2.  A small set of new, learnable vectors (the \"soft prompt\" or \"prompt embedding\") are added.\n",
        "3.  During training, only these few prompt vectors are updated using a small, high-quality dataset of expert-written examples (e.g., medical questions with ideal answers).\n",
        "4.  At inference time, these learned prompt vectors are prepended to the actual user input, guiding the frozen LLM to generate responses in the desired style (e.g., safe, informative, cautious).\n",
        "\n",
        "This is highly efficient in terms of both data and computation compared to full fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EducationalPromptTunedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified model to demonstrate the concept of prompt tuning.\n",
        "    \"\"\"\n",
        "    def __init__(self, frozen_llm, prompt_len=10, d_model=512):\n",
        "        super().__init__()\n",
        "        self.frozen_llm = frozen_llm\n",
        "        self.prompt_len = prompt_len\n",
        "        \n",
        "        # The ONLY trainable part of the model\n",
        "        self.soft_prompt = nn.Parameter(torch.randn(1, prompt_len, d_model))\n",
        "        \n",
        "        # Freeze the base LLM\n",
        "        for param in self.frozen_llm.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        # Prepend the learned soft prompt to the input embeddings\n",
        "        batch_size = input_embeddings.shape[0]\n",
        "        # Repeat the prompt for each item in the batch\n",
        "        prompt = self.soft_prompt.expand(batch_size, -1, -1)\n",
        "        \n",
        "        # Concatenate the soft prompt with the actual input\n",
        "        combined_input = torch.cat([prompt, input_embeddings], dim=1)\n",
        "        \n",
        "        # Pass the combined input through the frozen LLM\n",
        "        output = self.frozen_llm(combined_input)\n",
        "        return output\n",
        "\n",
        "# --- Example Usage ---\n",
        "# 1. Create a \"frozen\" LLM (a simple Transformer block in our case)\n",
        "frozen_llm = EducationalTransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
        "\n",
        "# 2. Create our prompt-tuned model\n",
        "med_palm_concept_model = EducationalPromptTunedModel(frozen_llm, prompt_len=20, d_model=512)\n",
        "\n",
        "# 3. Check which parameters are trainable\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "for name, param in med_palm_concept_model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        print(f\"Trainable parameter: {name} with size {param.shape}\")\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"\\nTotal Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")\n",
        "print(f\"Percentage of Trainable Params: {100 * trainable_params / total_params:.4f}%\")\n",
        "\n",
        "# This demonstrates the extreme parameter efficiency of the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Performer: Efficient Attention with Low-Rank Approximation\n",
        "\n",
        "For long biological sequences like proteins, the quadratic complexity of the $QK^T$ matrix multiplication in self-attention is a bottleneck. The Performer paper introduces a way to approximate the softmax attention kernel using a low-rank decomposition. \n",
        "\n",
        "Instead of computing the full $(N \\times N)$ attention matrix, it approximates it by mapping $Q$ and $K$ to a lower-dimensional randomized feature space. This changes the computation from $O(N^2 d)$ to $O(N r d)$, where $N$ is sequence length, $d$ is the feature dimension, and $r$ is the much smaller dimension of the random features ($r \\ll N$). This makes the computation linear in sequence length.\n",
        "\n",
        "The core idea relies on the fact that the softmax attention kernel can be expressed as: \n",
        "$$ \\text{Attention}(q, K, v)_i = E_{\\omega \\sim D}[\\phi(q_i)^T\\phi(K)^T v] $$\n",
        "where $\\phi$ is a feature map based on random projections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_performer_attention(Q, K, V, num_random_features):\n",
        "    \"\"\"\n",
        "    Educational implementation of the Performer's core idea.\n",
        "    - Approximates softmax attention with random feature maps.\n",
        "    - Avoids explicit computation of the (N x N) attention matrix.\n",
        "    \"\"\"\n",
        "    seq_len, d_k = Q.shape\n",
        "    \n",
        "    # 1. Create a random projection matrix\n",
        "    # In the actual paper, this is drawn from a specific distribution.\n",
        "    random_projection_matrix = torch.randn(d_k, num_random_features)\n",
        "    \n",
        "    # 2. Define the feature map phi(x). Here we use a simplified version.\n",
        "    # The paper uses exp(xW) with cos and sin features.\n",
        "    def feature_map(x):\n",
        "        # Project to the random feature space\n",
        "        projected = x @ random_projection_matrix\n",
        "        # A non-linearity to approximate the kernel\n",
        "        return F.relu(projected) / np.sqrt(num_random_features)\n",
        "\n",
        "    # 3. Map Q and K to the lower-dimensional space\n",
        "    Q_prime = feature_map(Q) # (seq_len, num_random_features)\n",
        "    K_prime = feature_map(K) # (seq_len, num_random_features)\n",
        "    \n",
        "    # 4. Compute the attention output without forming the QK^T matrix\n",
        "    # The key insight is to change the order of matrix multiplication\n",
        "    # Standard: (Q @ K.T) @ V\n",
        "    # Performer: Q @ (K.T @ V)\n",
        "    # This is possible because (A B) C = A (B C)\n",
        "    kv_product = K_prime.T @ V # (num_random_features, d_k)\n",
        "    output = Q_prime @ kv_product # (seq_len, d_k)\n",
        "    \n",
        "    return output\n",
        "\n",
        "# --- Comparison ---\n",
        "seq_len = 1024 # A longer sequence where the difference matters\n",
        "d_model = 64\n",
        "d_k = 32\n",
        "num_random_features = 128 # r << seq_len\n",
        "\n",
        "input_long_seq = torch.randn(seq_len, d_model)\n",
        "W_q = torch.randn(d_model, d_k)\n",
        "W_k = torch.randn(d_model, d_k)\n",
        "W_v = torch.randn(d_model, d_k)\n",
        "Q = input_long_seq @ W_q\n",
        "K = input_long_seq @ W_k\n",
        "V = input_long_seq @ W_v\n",
        "\n",
        "print(f\"Sequence Length (N): {seq_len}\")\n",
        "print(f\"Random Features (r): {num_random_features}\")\n",
        "print(\"\\n--- Theoretical Complexity ---\")\n",
        "print(f\"Standard Attention: O(N^2 * d) = O({seq_len**2 * d_k}) = O({seq_len**2 * d_k:,})\")\n",
        "print(f\"Performer Attention: O(N * r * d) = O({seq_len * num_random_features * d_k}) = O({seq_len * num_random_features * d_k:,})\")\n",
        "\n",
        "performer_output = educational_performer_attention(Q, K, V, num_random_features)\n",
        "print(\"\\nPerformer Output Shape:\", performer_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Enformer: Predicting Gene Expression with Transformers\n",
        "\n",
        "The Enformer model was designed to predict gene expression from DNA sequences, a task that requires modeling extremely long-range interactions (up to 100,000 base pairs away). As we saw earlier, CNNs are ill-suited for this. \n",
        "\n",
        "The Enformer architecture combines the strengths of both models:\n",
        "1.  **CNN Stem:** A few layers of CNNs at the beginning to learn local patterns and downsample the very long input sequence, making it computationally tractable for the Transformer layers.\n",
        "2.  **Transformer Body:** A stack of Transformer blocks that can then model the global, long-range interactions between the features extracted by the CNN stem.\n",
        "\n",
        "This hybrid approach allows the model to capture the influence of distant *enhancer* regions on gene *promoters*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EducationalEnformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified, educational version of the Enformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=128, n_heads=4, n_layers=2, d_ff=512):\n",
        "        super().__init__()\n",
        "        # In the real paper, input is one-hot encoded DNA (4 channels)\n",
        "        # Here, we'll assume a single channel for simplicity.\n",
        "        \n",
        "        # 1. CNN Stem to downsample and extract local features\n",
        "        self.cnn_stem = nn.Sequential(\n",
        "            # Conv -> Pool -> Conv -> Pool\n",
        "            nn.Conv1d(in_channels=1, out_channels=d_model//2, kernel_size=15, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=d_model//2, out_channels=d_model, kernel_size=15, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "        \n",
        "        # 2. Transformer Body to model long-range interactions\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            nhead=n_heads, \n",
        "            dim_feedforward=d_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_body = nn.TransformerEncoder(transformer_layer, num_layers=n_layers)\n",
        "        \n",
        "        # 3. Final head to predict gene expression\n",
        "        self.prediction_head = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, dna_sequence):\n",
        "        # Input shape: (batch_size, channels, seq_len)\n",
        "        \n",
        "        # Pass through CNN stem\n",
        "        x = self.cnn_stem(dna_sequence)\n",
        "        \n",
        "        # Reshape for Transformer: (batch_size, seq_len, d_model)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "        # Pass through Transformer body\n",
        "        x = self.transformer_body(x)\n",
        "        \n",
        "        # Use the representation of the central part of the sequence for prediction\n",
        "        # (A common technique in genomics models)\n",
        "        center_idx = x.shape[1] // 2\n",
        "        gene_representation = x[:, center_idx, :]\n",
        "        \n",
        "        # Final prediction\n",
        "        prediction = self.prediction_head(gene_representation)\n",
        "        return prediction\n",
        "\n",
        "# --- Example Usage ---\n",
        "batch_size = 4\n",
        "dna_length = 4096 # A much shorter sequence than the real paper (200k)\n",
        "\n",
        "model = EducationalEnformer()\n",
        "# Input needs shape (batch, channels, length)\n",
        "dummy_dna = torch.randn(batch_size, 1, dna_length)\n",
        "output = model(dummy_dna)\n",
        "\n",
        "print(\"Input DNA sequence shape:\", dummy_dna.shape)\n",
        "print(\"Final gene expression prediction shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Experimental Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Reproducing the Med-PaLM Evaluation Concept\n",
        "\n",
        "The Med-PaLM paper emphasized that standard metrics like accuracy are insufficient for medical applications. They developed a human evaluation framework where clinicians and lay users rated model responses on multiple axes. We can simulate this process interactively.\n",
        "\n",
        "Below, we compare a hypothetical \"unaligned\" FLAN-PaLM response with an \"aligned\" Med-PaLM response. Use the sliders to rate each response according to the criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_medical_qa_evaluation():\n",
        "    \"\"\"\n",
        "    Interactive widgets to simulate the evaluation of medical QA responses.\n",
        "    \"\"\"\n",
        "    question = \"**User Question:** I have a persistent cough and occasional shortness of breath. What could it be?\"\n",
        "    \n",
        "    flan_palm_answer = (\"**FLAN-PaLM (Unaligned) Style Answer:**\\n\" \n",
        "                        \"You have bronchitis. Take cough syrup.\")\n",
        "    \n",
        "    med_palm_answer = (\"**Med-PaLM (Aligned) Style Answer:**\\n\" \n",
        "                       \"A persistent cough and shortness of breath can have several potential causes, ranging from common respiratory infections to more serious conditions. It is not possible to provide a diagnosis without a full medical evaluation. Common causes include bronchitis, asthma, or even acid reflux. However, it's very important to see a healthcare professional to rule out other possibilities. They can perform a physical examination and may recommend further tests if needed. You should consult a doctor for an accurate diagnosis and treatment plan.\")\n",
        "\n",
        "    print(question)\n",
        "    print(\"-\"*50)\n",
        "    print(flan_palm_answer)\n",
        "    print(\"\\nRate the FLAN-PaLM Answer:\")\n",
        "    widgets.IntSlider(description='Factual?', min=1, max=5, value=2)\n",
        "    widgets.IntSlider(description='Helpful?', min=1, max=5, value=2)\n",
        "    widgets.IntSlider(description='Safe?', min=1, max=5, value=1)\n",
        "\n",
        "    # Create widgets for FLAN-PaLM ratings\n",
        "    style = {'description_width': 'initial'}\n",
        "    fp_factuality = widgets.IntSlider(description='Factuality (1-5)', min=1, max=5, value=2, style=style)\n",
        "    fp_helpfulness = widgets.IntSlider(description='Helpfulness (1-5)', min=1, max=5, value=2, style=style)\n",
        "    fp_safety = widgets.IntSlider(description='Potential for Harm (1=High, 5=Low)', min=1, max=5, value=1, style=style)\n",
        "    \n",
        "    # Create widgets for Med-PaLM ratings\n",
        "    mp_factuality = widgets.IntSlider(description='Factuality (1-5)', min=1, max=5, value=5, style=style)\n",
        "    mp_helpfulness = widgets.IntSlider(description='Helpfulness (1-5)', min=1, max=5, value=5, style=style)\n",
        "    mp_safety = widgets.IntSlider(description='Potential for Harm (1=High, 5=Low)', min=1, max=5, value=5, style=style)\n",
        "\n",
        "    # Display everything\n",
        "    display(fp_factuality, fp_helpfulness, fp_safety)\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(med_palm_answer)\n",
        "    print(\"\\nRate the Med-PaLM Answer:\")\n",
        "    display(mp_factuality, mp_helpfulness, mp_safety)\n",
        "    \n",
        "    print(\"\\nThis simulation highlights the qualitative gap that alignment techniques like Instruction Prompt Tuning aim to close.\")\n",
        "\n",
        "interactive_medical_qa_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Enformer Experiment: Comparing CNN vs. Transformer on a Toy Genomic Task\n",
        "\n",
        "We'll create a synthetic dataset to demonstrate the core finding of the Enformer paper: Transformers can model long-range dependencies that CNNs cannot.\n",
        "\n",
        "**Task:** Predict a binary output (gene expression: ON/OFF).\n",
        "**Rule:** The output is ON (1) if and only if a specific sequence pattern (\"promoter\") is present at the center of the sequence AND another specific pattern (\"enhancer\") is present far away. Otherwise, the output is OFF (0).\n",
        "\n",
        "A CNN with a small receptive field will struggle to see both patterns simultaneously, while a Transformer should succeed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_synthetic_genome_data(num_samples, seq_len, promoter, enhancer):\n",
        "    \"\"\"Creates a synthetic dataset for the Enformer experiment.\"\"\"\n",
        "    X = torch.randint(0, 4, (num_samples, seq_len)) # 4 bases: A, C, G, T\n",
        "    y = torch.zeros(num_samples, 1)\n",
        "    \n",
        "    center = seq_len // 2\n",
        "    enhancer_pos = 50\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        has_promoter = (np.random.rand() > 0.5)\n",
        "        has_enhancer = (np.random.rand() > 0.5)\n",
        "        \n",
        "        if has_promoter:\n",
        "            X[i, center:center+len(promoter)] = torch.tensor(promoter)\n",
        "        if has_enhancer:\n",
        "            X[i, enhancer_pos:enhancer_pos+len(enhancer)] = torch.tensor(enhancer)\n",
        "            \n",
        "        if has_promoter and has_enhancer:\n",
        "            y[i] = 1.0\n",
        "    \n",
        "    # Convert to one-hot encoding for models\n",
        "    X_onehot = F.one_hot(X, num_classes=4).float().permute(0, 2, 1)\n",
        "    return X_onehot, y\n",
        "\n",
        "# Simple CNN model for baseline comparison\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(4, 16, 7, padding='same')\n",
        "        self.pool = nn.MaxPool1d(4)\n",
        "        self.conv2 = nn.Conv1d(16, 32, 7, padding='same')\n",
        "        self.fc = nn.Linear(32 * (2048//16), 1) # Length is hardcoded after pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# --- Training Loop ---\n",
        "def train_model(model, data, labels, epochs=20):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "        preds = torch.sigmoid(model(data)) > 0.5\n",
        "        accuracy = (preds.float() == labels).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "print(\"Running toy Enformer experiment... This may take a minute.\")\n",
        "# Create data\n",
        "seq_len = 2048\n",
        "promoter = [0, 1, 2, 3] # ACGT\n",
        "enhancer = [3, 2, 1, 0] # TGCA\n",
        "X, y = create_synthetic_genome_data(512, seq_len, promoter, enhancer)\n",
        "\n",
        "# Train CNN\n",
        "cnn_model = SimpleCNN()\n",
        "cnn_acc = train_model(cnn_model, X, y, epochs=50)\n",
        "\n",
        "# Train Transformer (simplified Enformer)\n",
        "enformer_toy_model = EducationalEnformer(d_model=32, n_heads=4, n_layers=1, d_ff=128)\n",
        "# The CNN stem in EducationalEnformer takes 1 channel, so we average the one-hot\n",
        "X_enformer = X.mean(axis=1, keepdim=True)\n",
        "enformer_acc = train_model(enformer_toy_model, X_enformer, y, epochs=50)\n",
        "\n",
        "print(f\"\\n--- Results ---\")\n",
        "print(f\"Simple CNN Accuracy: {cnn_acc*100:.2f}% (Should be near random chance ~50-75%)\")\n",
        "print(f\"Simplified Enformer Accuracy: {enformer_acc*100:.2f}% (Should be much higher)\")\n",
        "print(\"\\nConclusion: The Transformer's global attention allows it to learn the long-range dependency, while the CNN fails.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Research Context & Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Contribution in Context\n",
        "\n",
        "The lecture by Vivek Natarajan positions these works as part of a larger trend: the convergence of AI methodologies around the Transformer architecture. \n",
        "\n",
        "- **Med-PaLM** builds on the foundation of general-purpose LLMs (PaLM, FLAN-PaLM) and adapts them to a specialized, high-stakes domain. Its contribution is less about architectural novelty and more about the crucial aspects of **data, evaluation, and safe alignment**.\n",
        "- **Performer and Enformer** are examples of architectural innovation driven by the specific constraints of biological data. They tackle the problem of **long sequences and long-range dependencies**, pushing the boundaries of what Transformers can efficiently process.\n",
        "- **ProtNLM and DeepConsensus** showcase the direct, practical application of established Transformer models (T5, standard encoder) to solve high-impact problems in protein annotation and genomics, demonstrating the versatility of the architecture.\n",
        "\n",
        "Together, they illustrate that applying Transformers to biomedicine requires a holistic approach, innovating not just on models, but also on data curation, evaluation frameworks, and loss functions tailored to the domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Current Research Directions Mentioned\n",
        "The lecture concluded by highlighting several key areas for future research:\n",
        "\n",
        "- **Multimodality:** The ultimate goal is to build foundational models that can process the full spectrum of biomedical data—text, genomics, proteomics, medical imaging—within a single, unified framework.\n",
        "- **Data Scarcity and Privacy:** Medical datasets are often small and siloed due to privacy regulations. Techniques like **federated learning and evaluation** will be critical to train and validate models without centralizing sensitive data.\n",
        "- **Improved Uncertainty & Reliability:** For clinical use, models must be able to reliably communicate when they are uncertain. Research into better methods for uncertainty quantification and the ability to **defer to an expert** is paramount.\n",
        "- **Retrieval Augmented Models:** Enhancing LLMs with the ability to retrieve information from authoritative, up-to-date sources (like medical textbooks or recent research papers) to reduce hallucination and provide citable evidence for their answers.\n",
        "- **Generalist vs. Specialist Models:** An ongoing debate is whether large, general-purpose LLMs fine-tuned for medicine will outperform smaller, specialist models trained from scratch on domain-specific data. The answer likely involves a combination of both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Applications Discussed\n",
        "The research presented is not merely academic; it points towards tangible real-world impact:\n",
        "\n",
        "- **Clinical Workflow Automation:** Near-term applications of models like Med-PaLM will likely focus on augmenting physicians by automating tasks like generating clinical note summaries, drafting insurance letters, or converting complex medical jargon into patient-friendly language.\n",
        "- **Accelerating Scientific Discovery:** Tools like ProtNLM can annotate millions of uncharacterized proteins, creating a massive, searchable database that could accelerate research in areas like drug discovery. Enformer-like models can prioritize genetic variants for experimental validation, saving time and resources.\n",
        "- **Rapid Diagnostics:** The use of DeepConsensus in a record-setting rapid genome sequencing case demonstrates the potential for AI to have a direct impact on patient outcomes by enabling faster, more accurate diagnoses for genetic conditions."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}