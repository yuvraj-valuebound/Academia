{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundation Models for Robotics: A Deep Dive into Google's Approach\n",
        "\n",
        "This notebook explores the concepts presented in Ted Xiao's lecture on the shift towards foundation models in robotics. We will deconstruct the philosophy, architecture, and experiments behind key projects like RT-1, SayCan, Inner Monologue, and DIAL. The goal is to provide an interactive, educational experience that follows the evolution of thought at Google's robotics team, moving from online Reinforcement Learning to large-scale, offline, language-guided imitation learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Overview & Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of the Research Paper/Lecture Topic\n",
        "\n",
        "This lecture outlines a paradigm shift in robot learning, advocating for the application of principles from foundation models (like large language models) to robotics. The core thesis is that scaling up data, model size, and compute—a recipe successful in other domains—can unlock emergent capabilities necessary for robots to operate in the complex, unstructured real world. The talk argues for a move from online, trial-and-error learning (Reinforcement Learning) to an offline paradigm where data collection and policy learning are decoupled. The proposed recipe for modern embodied intelligence is: **Large, diverse offline datasets + High-capacity architectures (Transformers) + Language as a universal glue.**\n",
        "\n",
        "The lecture details several key projects that embody this philosophy:\n",
        "- **RT-1 (Robotics Transformer 1):** A Transformer-based architecture for multi-task imitation learning that demonstrates robustness and scalability with diverse data.\n",
        "- **SayCan:** A framework that uses large language models (LLMs) for high-level, long-horizon planning, grounding the LLM's suggestions with the robot's learned skills (affordances).\n",
        "- **Inner Monologue:** An extension of SayCan that incorporates closed-loop feedback from the environment using Vision-Language Models (VLMs) to enable dynamic replanning and error recovery.\n",
        "- **DIAL:** A data-centric method that uses VLMs to automatically relabel a large, templated robotics dataset with rich, natural language descriptions, significantly enhancing the policy's language understanding and generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite Mathematical Concepts\n",
        "\n",
        "- **Self-Attention Mechanism:** The core component of the Transformer architecture, allowing the model to weigh the importance of different tokens in an input sequence.\n",
        "- **Cross-Entropy Loss:** A standard loss function for classification problems, used here for predicting discretized robot actions from a fixed set of possibilities.\n",
        "- **Value Functions (Q-Learning):** Functions that estimate the expected return (reward) of taking an action in a given state. Used conceptually in SayCan to represent the robot's confidence or `affordance` for a specific skill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite ML/CS Concepts\n",
        "\n",
        "- **Transformer Architecture:** Understanding of the encoder-decoder structure, tokenization, embeddings, and the role of self-attention.\n",
        "- **Imitation Learning (Behavioral Cloning):** The supervised learning approach of training a policy to mimic expert demonstrations (mapping observations to actions).\n",
        "- **Offline vs. Online Reinforcement Learning:** Understanding the distinction between learning from a static, pre-collected dataset (offline) versus learning through active, real-time interaction with an environment (online).\n",
        "- **Large Language Models (LLMs):** Familiarity with the capabilities of models like PaLM or GPT-3, particularly their use in few-shot prompting for planning and reasoning.\n",
        "- **Vision-Language Models (VLMs):** Familiarity with models like CLIP that can connect text and images, used for tasks like visual question answering (VQA) and image captioning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchy of Topics\n",
        "\n",
        "1.  **The Philosophical Shift:** Why Foundation Models? The Bitter Lessons 1.0 & 2.0.\n",
        "2.  **Mathematical Foundations:** Implementing Self-Attention from scratch.\n",
        "3.  **Prerequisite Algorithm:** A simple implementation of Behavioral Cloning (BC).\n",
        "4.  **Core Research - RT-1:** Deconstructing the Robotics Transformer architecture for robust skill learning.\n",
        "5.  **Core Research - SayCan & Inner Monologue:** Using LLMs and VLMs for planning and feedback.\n",
        "6.  **Core Research - DIAL:** Scaling data diversity through automatic VLM-based relabeling.\n",
        "7.  **Experimental Analysis:** Reproducing the key findings and ablation studies discussed in the lecture.\n",
        "8.  **Context & Extensions:** The broader implications and future of this research direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Objectives\n",
        "\n",
        "- Understand the motivation behind applying foundation model principles to robotics.\n",
        "- Grasp the architecture of the RT-1 model and its key design choices (e.g., action discretization).\n",
        "- Implement the core logic of grounding LLM planners with robotic affordances (SayCan).\n",
        "- Appreciate how VLMs can provide closed-loop feedback and augment training data.\n",
        "- Analyze the trade-offs between data quantity, task diversity, and model size based on the lecture's experimental results.\n",
        "\n",
        "**Estimated Time:** 2-3 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Mathematical Foundations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Transformer's Core: Self-Attention\n",
        "\n",
        "The lecture credits high-capacity architectures like the Transformer as a key ingredient for success. The magic of the Transformer lies in the self-attention mechanism, which allows the model to dynamically weigh the importance of different parts of the input sequence. For a robot, this could mean paying more attention to the object it needs to grasp rather than the distracting background. \n",
        "\n",
        "The formula for scaled dot-product attention is:\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
        "\n",
        "Where $Q$ (Query), $K$ (Key), and $V$ (Value) are learned linear projections of the input tokens. Let's implement this from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ipywidgets import interact, FloatSlider\n",
        "\n",
        "def educational_scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Clear implementation of scaled dot-product attention for understanding.\n",
        "    - Based directly on the mathematical formulation.\n",
        "    - Extensive comments explaining each step.\n",
        "    \"\"\"\n",
        "    # Get the dimension of the key vectors\n",
        "    d_k = K.shape[-1]\n",
        "    \n",
        "    # Step 1: Compute the dot products of the query with all keys (QK^T).\n",
        "    # This gives us a raw score of how much each token should attend to every other token.\n",
        "    # Shape: (batch_size, num_queries, num_keys)\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    \n",
        "    # Step 2: Scale the attention scores by the square root of the key dimension.\n",
        "    # This prevents the gradients from becoming too small, especially for large d_k.\n",
        "    scaled_attention_scores = attention_scores / np.sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Apply softmax to get the attention weights.\n",
        "    # This converts the scores into a probability distribution, where weights sum to 1.\n",
        "    # Shape: (batch_size, num_queries, num_keys)\n",
        "    attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
        "    \n",
        "    # Step 4: Multiply the weights by the value vectors.\n",
        "    # This creates a weighted sum of the values, where tokens that are more \"important\"\n",
        "    # (have higher attention weights) contribute more to the output.\n",
        "    # Shape: (batch_size, num_queries, value_dim)\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# --- Let's create some dummy data to test it ---\n",
        "batch_size = 1\n",
        "sequence_length = 5 # e.g., 1 language token + 4 image patch tokens\n",
        "embedding_dim = 64 # Dimension of each token\n",
        "\n",
        "# In a real transformer, Q, K, and V are projections of the same input sequence.\n",
        "input_sequence = torch.randn(batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "# For simplicity, let's treat the input as Q, K, and V directly.\n",
        "Q, K, V = input_sequence, input_sequence, input_sequence\n",
        "\n",
        "output, attention_weights = educational_scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Input shape:\", input_sequence.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention weights shape:\", attention_weights.shape)\n",
        "\n",
        "# Visualize the attention weights\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(attention_weights.squeeze(0).detach().numpy(), annot=True, cmap='viridis')\n",
        "plt.xlabel(\"Key Tokens\")\n",
        "plt.ylabel(\"Query Tokens\")\n",
        "plt.title(\"Self-Attention Weights Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Value Functions as Affordances\n",
        "The SayCan paper uses a `value function` to represent the robot's `affordance` for a task. In simple terms, this is a score from the robot policy that says, \"Given the current world state (what I see), how likely am I to succeed at task X?\" A high value means high confidence. This is crucial for grounding the LLM's abstract plans in the robot's actual capabilities.\n",
        "\n",
        "Let's create a mock value function to simulate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_mock_affordance_function(state, possible_skills):\n",
        "    \"\"\"\n",
        "    Simulates a robot's value function to estimate success probability for skills.\n",
        "    - In a real system, this would be a learned neural network.\n",
        "    - Here, we use simple heuristics for educational purposes.\n",
        "    \"\"\"\n",
        "    affordances = {}\n",
        "    # The 'state' is a simple dictionary describing what the robot sees.\n",
        "    objects_in_scene = state.get('objects', [])\n",
        "    robot_hand_empty = state.get('hand_empty', True)\n",
        "    \n",
        "    print(f\"\\nRobot State: {state}\")\n",
        "    \n",
        "    for skill in possible_skills:\n",
        "        # Default confidence is low.\n",
        "        confidence = 0.1\n",
        "        \n",
        "        # Heuristics to determine confidence\n",
        "        if 'pick' in skill:\n",
        "            # Can only pick an object if it's in the scene and hand is empty\n",
        "            target_object = skill.split(' ')[1]\n",
        "            if target_object in objects_in_scene and robot_hand_empty:\n",
        "                confidence = 0.9\n",
        "            else:\n",
        "                confidence = 0.05 # Very low confidence if conditions aren't met\n",
        "        \n",
        "        elif 'place' in skill:\n",
        "            # Can only place if the hand is not empty\n",
        "            if not robot_hand_empty:\n",
        "                confidence = 0.85\n",
        "        \n",
        "        elif 'find' in skill:\n",
        "            # Robot can always try to find things\n",
        "            confidence = 0.7\n",
        "        \n",
        "        affordances[skill] = confidence\n",
        "        \n",
        "    return affordances\n",
        "\n",
        "# --- Example Usage ---\n",
        "robot_skills = ['pick apple', 'pick coke_can', 'place on_table', 'find sponge']\n",
        "\n",
        "# Scenario 1: Robot sees an apple, hand is empty\n",
        "current_state_1 = {'objects': ['apple', 'table'], 'hand_empty': True}\n",
        "affordances_1 = educational_mock_affordance_function(current_state_1, robot_skills)\n",
        "print(\"Affordances (Scenario 1):\", affordances_1)\n",
        "\n",
        "# Scenario 2: Robot is holding something, sees a table\n",
        "current_state_2 = {'objects': ['table'], 'hand_empty': False}\n",
        "affordances_2 = educational_mock_affordance_function(current_state_2, robot_skills)\n",
        "print(\"Affordances (Scenario 2):\", affordances_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Prerequisite Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Behavioral Cloning (BC)\n",
        "\n",
        "The lecture highlights the team's consolidation around multi-task imitation learning. The simplest form of this is Behavioral Cloning (BC). It treats robot learning as a supervised learning problem: given a large dataset of `(observation, action)` pairs from an expert (e.g., a human teleoperator), we train a policy network $\\pi(action | observation)$ to predict the expert's action given an observation. The RT-1 model is fundamentally a large, sophisticated BC policy.\n",
        "\n",
        "Let's create a toy example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def educational_behavioral_cloning():\n",
        "    \"\"\"\n",
        "    A clear, from-scratch implementation of Behavioral Cloning for understanding.\n",
        "    - Creates a simple dataset.\n",
        "    - Defines a simple neural network policy.\n",
        "    - Trains the policy to mimic the 'expert' data.\n",
        "    \"\"\"\n",
        "    # Step 1: Create a synthetic dataset of (observation, action) pairs.\n",
        "    # Imagine a robot trying to move to a target on a 1D line.\n",
        "    # Observation: [current_position, target_position]\n",
        "    # Action: [velocity]\n",
        "    num_samples = 1000\n",
        "    # Observations: current position is random, target is random\n",
        "    observations = torch.rand(num_samples, 2) * 10  # values between 0 and 10\n",
        "    \n",
        "    # Expert actions: move towards the target. Action = target - current\n",
        "    actions = observations[:, 1] - observations[:, 0]\n",
        "    actions = actions.unsqueeze(1) # Reshape for the model\n",
        "    \n",
        "    # Create a PyTorch DataLoader\n",
        "    dataset = TensorDataset(observations, actions)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    \n",
        "    # Step 2: Define the policy network.\n",
        "    # A simple Multi-Layer Perceptron (MLP).\n",
        "    policy_net = nn.Sequential(\n",
        "        nn.Linear(2, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1) # Output a single action value\n",
        "    )\n",
        "    \n",
        "    # Step 3: Define loss function and optimizer.\n",
        "    # Mean Squared Error is common for continuous actions.\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "    \n",
        "    # Step 4: The training loop.\n",
        "    print(\"Starting Behavioral Cloning training...\")\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for obs_batch, act_batch in dataloader:\n",
        "            # Forward pass: get the policy's predicted action\n",
        "            predicted_actions = policy_net(obs_batch)\n",
        "            \n",
        "            # Compute the loss between predicted and expert actions\n",
        "            loss = loss_fn(predicted_actions, act_batch)\n",
        "            \n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    print(\"\\nTraining complete.\")\n",
        "    return policy_net\n",
        "\n",
        "bc_policy = educational_behavioral_cloning()\n",
        "\n",
        "# --- Test the trained policy ---\n",
        "test_obs = torch.tensor([[2.0, 8.0]]) # Should move right (positive action)\n",
        "expert_action = test_obs[0, 1] - test_obs[0, 0]\n",
        "predicted_action = bc_policy(test_obs)\n",
        "print(f\"\\nTest Observation: {test_obs.numpy()}\")\n",
        "print(f\"Expert Action: {expert_action.item():.2f}\")\n",
        "print(f\"BC Policy Predicted Action: {predicted_action.item():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Core Research Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RT-1: Robotics Transformer\n",
        "\n",
        "RT-1 is a decoder-only Transformer that takes a sequence of images and a natural language instruction as input and outputs a discretized action. Let's outline its architecture conceptually based on the lecture.\n",
        "\n",
        "**Key Components:**\n",
        "1.  **Image Tokenizer (FiLM-EfficientNet):** An EfficientNet model (pre-trained on vision tasks) processes each image in a history of 6 images. Each image is split into patches. The language instruction is used to condition the EfficientNet's features using FiLM (Feature-wise Linear Modulation) layers. This helps the vision system focus on instruction-relevant features early on.\n",
        "2.  **TokenLearner:** To manage the computational cost of processing many image patches over a history of images, TokenLearner dynamically selects the most relevant 8 image patch tokens (out of 81) at each timestep. This is a learned attention mechanism over the patches.\n",
        "3.  **Language Tokenizer:** A standard text encoder (e.g., from T5) converts the natural language instruction into a sequence of embeddings.\n",
        "4.  **Transformer:** The selected image tokens and language tokens are fed into a decoder-only Transformer. It uses causal self-attention to predict the next action token.\n",
        "5.  **Action Discretization:** Instead of predicting continuous 7-DoF end-effector poses, RT-1 predicts a single token from a vocabulary of 256 discrete bins for each dimension of the action space. This turns the regression problem into a classification problem, which is often more stable to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EducationalRT1(nn.Module):\n",
        "    \"\"\"\n",
        "    A conceptual PyTorch implementation of the RT-1 architecture.\n",
        "    - Focuses on the data flow and interaction between components.\n",
        "    - Uses placeholders for complex modules like TokenLearner and EfficientNet.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_action_bins=256, action_dim=7, embedding_dim=128, num_heads=4, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.num_action_bins = num_action_bins\n",
        "        self.action_dim = action_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # 1. Placeholder for Image Tokenizer (FiLM-EfficientNet)\n",
        "        # This would take an image (e.g., 3x224x224) and language embedding and output patch embeddings\n",
        "        # Let's assume it outputs 81 patches of `embedding_dim`\n",
        "        self.image_tokenizer = nn.Linear(3*224*224, 81 * embedding_dim) # Highly simplified!\n",
        "        print(\"Component [1]: Image Tokenizer (Placeholder) Initialized.\")\n",
        "\n",
        "        # 2. Placeholder for TokenLearner\n",
        "        # This would take 81 patch tokens and select the top 8\n",
        "        # We simulate this by simply selecting the first 8 tokens\n",
        "        self.token_learner = lambda x: x[:, :8, :] # Simplified selector\n",
        "        print(\"Component [2]: TokenLearner (Placeholder) Initialized.\")\n",
        "\n",
        "        # 3. Placeholder for Language Tokenizer\n",
        "        # A real one would use a pre-trained model like BERT or T5.\n",
        "        self.language_embedding = nn.Embedding(num_embeddings=100, embedding_dim=embedding_dim) # vocab size = 100\n",
        "        print(\"Component [3]: Language Tokenizer (Placeholder) Initialized.\")\n",
        "\n",
        "        # 4. The Transformer Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        print(\"Component [4]: Transformer Decoder Initialized.\")\n",
        "\n",
        "        # 5. The Action Prediction Head\n",
        "        # Predicts a distribution over bins for EACH action dimension\n",
        "        self.action_head = nn.Linear(embedding_dim, action_dim * num_action_bins)\n",
        "        print(\"Component [5]: Action Prediction Head Initialized.\")\n",
        "\n",
        "    def forward(self, image_history, language_instruction):\n",
        "        \"\"\"\n",
        "        image_history: (batch, history_len, C, H, W) e.g., (1, 6, 3, 224, 224)\n",
        "        language_instruction: (batch, seq_len) e.g., (1, 10)\n",
        "        \"\"\"\n",
        "        batch_size = image_history.shape[0]\n",
        "        history_len = image_history.shape[1]\n",
        "        \n",
        "        # Process language instruction\n",
        "        lang_tokens = self.language_embedding(language_instruction)\n",
        "        # lang_tokens shape: (batch_size, lang_seq_len, embedding_dim)\n",
        "\n",
        "        # Process image history\n",
        "        image_tokens_list = []\n",
        "        for i in range(history_len):\n",
        "            img = image_history[:, i, :, :, :].view(batch_size, -1)\n",
        "            # Pass through tokenizer (FiLM would also take lang_tokens here)\n",
        "            patch_embeddings = self.image_tokenizer(img).view(batch_size, 81, self.embedding_dim)\n",
        "            # Pass through TokenLearner\n",
        "            selected_patches = self.token_learner(patch_embeddings)\n",
        "            image_tokens_list.append(selected_patches)\n",
        "        \n",
        "        # Concatenate all tokens: [lang, img1_p1..p8, img2_p1..p8, ...]\n",
        "        image_tokens = torch.cat(image_tokens_list, dim=1)\n",
        "        # Memory for the decoder (what it attends to)\n",
        "        memory = torch.cat([lang_tokens, image_tokens], dim=1)\n",
        "\n",
        "        # The decoder takes a target sequence. For inference, this starts with a special [START] token\n",
        "        # and we predict autoregressively. For simplicity, we'll just use a dummy target.\n",
        "        # The target for a decoder is what it's trying to predict.\n",
        "        # We use a single learnable embedding as the initial query to predict the first token.\n",
        "        tgt = torch.randn(batch_size, 1, self.embedding_dim)\n",
        "        \n",
        "        # The transformer decoder will attend to the `memory` (images + language)\n",
        "        transformer_output = self.transformer_decoder(tgt, memory)\n",
        "        \n",
        "        # The output of the transformer is used to predict the action\n",
        "        action_logits = self.action_head(transformer_output)\n",
        "        \n",
        "        # Reshape to (batch, action_dim, num_bins)\n",
        "        action_logits = action_logits.view(batch_size, self.action_dim, self.num_action_bins)\n",
        "        \n",
        "        return action_logits\n",
        "\n",
        "# --- Instantiate and test the model with dummy data ---\n",
        "rt1_model = EducationalRT1()\n",
        "dummy_images = torch.randn(1, 6, 3, 224, 224) # batch=1, history=6\n",
        "dummy_instruction = torch.randint(0, 100, (1, 10)) # batch=1, 10 words\n",
        "\n",
        "predicted_action_logits = rt1_model(dummy_images, dummy_instruction)\n",
        "print(f\"\\nOutput Logits Shape: {predicted_action_logits.shape}\")\n",
        "print(\"(Batch, Action_Dimension, Num_Bins)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SayCan: Grounding LLMs in Reality\n",
        "\n",
        "The core idea of SayCan is to combine the semantic common-sense knowledge of an LLM with the practical, learned knowledge of a robot policy. An LLM might suggest many plausible steps to solve a problem, but most are not physically possible for the robot. SayCan filters these suggestions.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Prompt an LLM:** Give a high-level instruction (e.g., \"I spilled my drink, can you help?\") to an LLM, along with a list of the robot's available low-level skills (e.g., `['find sponge', 'pick sponge', 'wipe table']`).\n",
        "2.  **Get LLM Scores:** The LLM assigns a probability to each skill, indicating how semantically relevant it is as the next step.\n",
        "3.  **Get Affordance Scores:** The robot's policy (a value function) looks at the current scene and scores each skill based on its predicted success probability (affordance).\n",
        "4.  **Combine Scores:** Multiply the LLM score and the affordance score for each skill.\n",
        "5.  **Select Action:** The skill with the highest combined score is chosen and executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def educational_mock_llm(instruction, possible_skills):\n",
        "    \"\"\"\n",
        "    Simulates an LLM providing semantic scores for the next best step.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    # Simple keyword matching to simulate LLM reasoning\n",
        "    if 'apple' in instruction and 'table' in instruction:\n",
        "        scores['find apple'] = 0.9\n",
        "        scores['pick apple'] = 0.8 # Less likely first step than finding\n",
        "        scores['place on_table'] = 0.5 # Can't place before picking\n",
        "    elif 'coke' in instruction:\n",
        "        scores['find coke_can'] = 0.85\n",
        "        scores['pick coke_can'] = 0.75\n",
        "    else:\n",
        "        # Default uniform distribution if instruction is not understood\n",
        "        for skill in possible_skills: scores[skill] = 1.0 / len(possible_skills)\n",
        "    \n",
        "    # Normalize to make it a probability distribution\n",
        "    total_score = sum(scores.values())\n",
        "    normalized_scores = {k: v / total_score for k, v in scores.items()}\n",
        "    return normalized_scores\n",
        "\n",
        "def educational_saycan_planner(instruction, robot_state, possible_skills):\n",
        "    \"\"\"\n",
        "    Implements the core logic of the SayCan planner.\n",
        "    \"\"\"\n",
        "    # Step 1 & 2: Get scores from the LLM\n",
        "    llm_scores = educational_mock_llm(instruction, possible_skills)\n",
        "    \n",
        "    # Step 3: Get scores from the robot's affordance function\n",
        "    affordance_scores = educational_mock_affordance_function(robot_state, possible_skills)\n",
        "    \n",
        "    # Step 4: Combine the scores\n",
        "    combined_scores = {}\n",
        "    for skill in possible_skills:\n",
        "        # Ensure all skills are present in both dictionaries\n",
        "        llm_score = llm_scores.get(skill, 0.0)\n",
        "        affordance_score = affordance_scores.get(skill, 0.0)\n",
        "        combined_scores[skill] = llm_score * affordance_score\n",
        "    \n",
        "    # Step 5: Select the best action\n",
        "    if not combined_scores:\n",
        "        return \"No valid action\", None\n",
        "    \n",
        "    best_action = max(combined_scores, key=combined_scores.get)\n",
        "    \n",
        "    # --- For visualization ---\n",
        "    results = pd.DataFrame([\n",
        "        llm_scores,\n",
        "        affordance_scores,\n",
        "        combined_scores\n",
        "    ], index=['LLM Score (Can)', 'Affordance Score (Say)', 'Combined Score (SayCan)'])\n",
        "    results = results.T.sort_values(by='Combined Score (SayCan)', ascending=False)\n",
        "    print(\"\\n--- SayCan Decision Matrix ---\")\n",
        "    print(results)\n",
        "    \n",
        "    return best_action\n",
        "\n",
        "# --- Example Usage: \"put an apple on the table\" ---\n",
        "high_level_instruction = \"put an apple on the table\"\n",
        "robot_skills = ['find apple', 'pick apple', 'place on_table', 'find coke_can']\n",
        "\n",
        "# State 1: Robot hand is empty, doesn't see an apple yet.\n",
        "state_1 = {'objects': ['table'], 'hand_empty': True}\n",
        "print(f\"\\nExecuting SayCan for instruction: '{high_level_instruction}'\")\n",
        "action_1 = educational_saycan_planner(high_level_instruction, state_1, robot_skills)\n",
        "print(f\"\\n==> Selected Action 1: {action_1}\")\n",
        "\n",
        "# State 2: Robot has found and picked the apple.\n",
        "state_2 = {'objects': ['table', 'apple'], 'hand_empty': False}\n",
        "print(f\"\\nExecuting SayCan for instruction: '{high_level_instruction}'\")\n",
        "action_2 = educational_saycan_planner(high_level_instruction, state_2, robot_skills)\n",
        "print(f\"\\n==> Selected Action 2: {action_2}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inner Monologue: Adding Closed-Loop Feedback\n",
        "\n",
        "Inner Monologue improves upon SayCan by making the planning process dynamic. Instead of a one-shot plan, it creates a continuous dialogue where the robot gets feedback from the environment, allowing it to recover from errors and react to changes.\n",
        "\n",
        "**Key Feedback Mechanisms:**\n",
        "1.  **Passive Scene Description:** A VLM captions the scene (e.g., \"I see a coke can and an apple on the table.\") and adds this text to the LLM's context.\n",
        "2.  **Success Detection:** After executing an action, a VLM-based classifier determines if it succeeded or failed. This result (\"Success: picked up the coke can\" or \"Failure: failed to pick up the coke can\") is fed back to the LLM.\n",
        "3.  **Active Scene Description (VQA):** The LLM can ask questions about the scene if it's uncertain (e.g., \"Is the drawer open?\"), which are answered by a VQA model or a human."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_inner_monologue():\n",
        "    \"\"\"\n",
        "    Simulates a long-horizon task using the Inner Monologue loop.\n",
        "    \"\"\"\n",
        "    # -- Mocks for VLM components --\n",
        "    def mock_scene_describer(state):\n",
        "        return f\"Scene description: I see {', '.join(state['objects'])}. My hand is {'empty' if state['hand_empty'] else 'holding something'}.\"\n",
        "    \n",
        "    def mock_success_detector(action, state_before, state_after):\n",
        "        if 'pick' in action:\n",
        "            return not state_after['hand_empty'] # Success if hand is not empty after picking\n",
        "        if 'place' in action:\n",
        "            return state_after['hand_empty'] # Success if hand is empty after placing\n",
        "        return True # Assume other actions always succeed\n",
        "\n",
        "    # -- Simulation Setup --\n",
        "    instruction = \"pick up the coke can\"\n",
        "    skills = ['find coke_can', 'pick coke_can', 'place on_table']\n",
        "    robot_state = {'objects': ['table', 'coke_can'], 'hand_empty': True}\n",
        "    llm_context = [f\"User instruction: {instruction}\"]\n",
        "    max_steps = 3\n",
        "\n",
        "    print(\"--- Starting Inner Monologue Execution ---\")\n",
        "    for step in range(max_steps):\n",
        "        print(f\"\\n--- Step {step+1} ---\")\n",
        "        # 1. Add passive scene description to context\n",
        "        scene_text = mock_scene_describer(robot_state)\n",
        "        llm_context.append(scene_text)\n",
        "        print(f\"LLM Context: ... {llm_context[-1]}\")\n",
        "\n",
        "        # 2. Run SayCan to get the next action\n",
        "        # (Passing the full context to the LLM would be ideal, we simulate its effect)\n",
        "        action_to_execute = educational_saycan_planner(instruction, robot_state, skills)\n",
        "        print(f\"Selected Action: {action_to_execute}\")\n",
        "\n",
        "        # 3. Simulate executing the action and detecting success\n",
        "        state_before = robot_state.copy()\n",
        "        \n",
        "        # Simulate a failure on the first attempt!\n",
        "        if action_to_execute == 'pick coke_can' and state_before['hand_empty']:\n",
        "            print(\"\\033[91mSimulating a grasp failure!\\033[0m\")\n",
        "            success = False\n",
        "            state_after = state_before # State does not change\n",
        "        else: # On retry, it will succeed\n",
        "            success = True\n",
        "            robot_state['hand_empty'] = False\n",
        "            state_after = robot_state.copy()\n",
        "\n",
        "        # 4. Add success/failure feedback to context\n",
        "        feedback = f\"Executed '{action_to_execute}'. Result: {'Success' if success else 'Failure'}.\"\n",
        "        llm_context.append(feedback)\n",
        "        print(feedback)\n",
        "        robot_state = state_after\n",
        "        \n",
        "        if not success:\n",
        "            print(\"\\nAction failed. The LLM will now use this feedback to replan.\")\n",
        "        else:\n",
        "            print(\"\\nAction succeeded.\")\n",
        "            break # End simulation after success\n",
        "\n",
        "educational_inner_monologue()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DIAL: Data-Informed Augmentation of Language\n",
        "\n",
        "A key bottleneck is the cost of collecting and labeling data. The original dataset for RT-1 used 700 templated instructions (e.g., \"pick coke can\"). This limits the robot's ability to understand natural, varied language. DIAL addresses this by using a VLM to automatically relabel the entire dataset.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Collect a small, high-quality labeled set:** A small fraction (3%) of the data is manually labeled with rich, descriptive captions (e.g., \"lift up the red coke can that is next to the bag of chips\").\n",
        "2.  **Fine-tune a VLM:** A pre-trained VLM (like CLIP) is fine-tuned on this small dataset to become a specialized robot trajectory captioner.\n",
        "3.  **Pseudo-label the large dataset:** The fine-tuned VLM is used to generate new, descriptive labels for the remaining 97% of the dataset that only had templated labels.\n",
        "4.  **Train the policy:** The RT-1 model is trained on the combined dataset (original templated data + human-labeled data + VLM pseudo-labeled data), vastly increasing the linguistic diversity of its training signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def educational_dial_relabeling():\n",
        "    \"\"\"\n",
        "    Simulates the DIAL process of relabeling a dataset with a VLM.\n",
        "    \"\"\"\n",
        "    # Step 1: The original dataset with templated labels\n",
        "    original_dataset = {\n",
        "        'trajectory_001': {'template_label': 'pick coke_can', 'visuals': 'coke can is on the left'},\n",
        "        'trajectory_002': {'template_label': 'pick coke_can', 'visuals': 'coke can is red, near chips'},\n",
        "        'trajectory_003': {'template_label': 'open drawer', 'visuals': 'top drawer is being opened'},\n",
        "        'trajectory_004': {'template_label': 'pick apple', 'visuals': 'green apple picked from bowl'}\n",
        "    }\n",
        "    print(\"--- Original Dataset ---\")\n",
        "    for traj, data in original_dataset.items():\n",
        "        print(f\"{traj}: '{data['template_label']}'\")\n",
        "        \n",
        "    # Step 2 & 3: Simulate a fine-tuned VLM generating new labels\n",
        "    # The VLM takes the trajectory's visuals and outputs a rich caption.\n",
        "    def mock_finetuned_vlm(visuals):\n",
        "        if 'left' in visuals:\n",
        "            return \"pick up the coke can from the left side of the table\"\n",
        "        if 'red' in visuals and 'chips' in visuals:\n",
        "            return \"get the red can of coke that is next to the chip bag\"\n",
        "        if 'top drawer' in visuals:\n",
        "            return \"hold and pull out the top drawer\"\n",
        "        if 'green apple' in visuals:\n",
        "            return \"lift the green apple from the white bowl\"\n",
        "        return \"unknown action\"\n",
        "        \n",
        "    # Apply the VLM to relabel the dataset\n",
        "    relabeling_results = {}\n",
        "    for traj_id, data in original_dataset.items():\n",
        "        new_label = mock_finetuned_vlm(data['visuals'])\n",
        "        relabeling_results[traj_id] = new_label\n",
        "        \n",
        "    print(\"\\n--- VLM Relabeling Results ---\")\n",
        "    for traj, new_label in relabeling_results.items():\n",
        "        print(f\"{traj}: '{new_label}'\")\n",
        "        \n",
        "    # Step 4: The new training data for RT-1 now includes these diverse labels\n",
        "    print(\"\\nThis enriched dataset is now used to train the RT-1 policy,\")\n",
        "    print(\"improving its generalization to novel language instructions.\")\n",
        "    \n",
        "educational_dial_relabeling()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Experimental Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RT-1: Data Size vs. Task Diversity\n",
        "\n",
        "The lecture presented a key finding from the RT-1 experiments: **task diversity is more important than the number of demonstrations per task.** Reducing the number of unique tasks hurt performance more than proportionally reducing the number of episodes for every task. This suggests that for robots to scale, collecting a wider variety of behaviors is more critical than collecting more data for the same few behaviors.\n",
        "\n",
        "Let's visualize this finding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rt1_scaling_ablation():\n",
        "    \"\"\"\n",
        "    Reproduces the conceptual plot from the RT-1 ablation study.\n",
        "    \"\"\"\n",
        "    # Percentage of total data used\n",
        "    data_fraction = np.array([100, 75, 50, 25, 10])\n",
        "    \n",
        "    # Max success rate\n",
        "    baseline_success = 85\n",
        "    \n",
        "    # Performance when reducing episodes per task (less data, same diversity)\n",
        "    # Performance degrades gracefully.\n",
        "    perf_reduce_episodes = baseline_success * (data_fraction / 100)**0.5\n",
        "    \n",
        "    # Performance when reducing the number of unique tasks (less diversity)\n",
        "    # Performance degrades much more sharply.\n",
        "    perf_reduce_tasks = baseline_success * (data_fraction / 100)**1.2\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data_fraction, perf_reduce_episodes, marker='o', linestyle='--', label='Reducing Demos per Task (High Diversity)')\n",
        "    plt.plot(data_fraction, perf_reduce_tasks, marker='s', linestyle='--', label='Reducing Number of Tasks (Low Diversity)')\n",
        "    \n",
        "    plt.title('RT-1 Ablation: Task Diversity vs. Data per Task')\n",
        "    plt.xlabel('Percentage of Training Data Used')\n",
        "    plt.ylabel('Success Rate (%)')\n",
        "    plt.gca().invert_xaxis() # Match the lecture's style\n",
        "    plt.grid(True, which='both', linestyle='-', linewidth=0.5)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_rt1_scaling_ablation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SayCan: Scaling with Better Foundation Models\n",
        "\n",
        "Another key insight was related to the \"Bitter Lesson 2.0\": we should leverage methods that improve as foundation models improve. The SayCan experiment demonstrated this perfectly. When the team hot-swapped the underlying LLM from a smaller model (FLAN) to a much larger one (PaLM), the high-level planning success rate increased significantly, without changing anything else about the robot or its skills.\n",
        "\n",
        "This shows the power of decoupling the planning module; as external foundation models get better, the robot's capabilities improve for free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_saycan_llm_scaling():\n",
        "    \"\"\"\n",
        "    Visualizes the improvement in planning success with larger LLMs.\n",
        "    \"\"\"\n",
        "    # Model sizes are illustrative, based on known model scales\n",
        "    model_params = np.array([8, 62, 540]) # Billions of parameters (e.g., small, FLAN, PaLM)\n",
        "    planning_success_rate = np.array([75, 84, 95])\n",
        "    execution_success_rate = np.array([65, 74, 84]) # Overall success is planning * execution\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(model_params, planning_success_rate, marker='o', linestyle='--', label='Planning Success Rate')\n",
        "    plt.plot(model_params, execution_success_rate, marker='s', linestyle='--', label='Overall Execution Success Rate')\n",
        "\n",
        "    plt.title('SayCan: Performance Scales with LLM Size (Bitter Lesson 2.0)')\n",
        "    plt.xlabel('LLM Parameters (Billions)')\n",
        "    plt.ylabel('Success Rate (%)')\n",
        "    plt.xscale('log')\n",
        "    plt.xticks(model_params, [f'{x}B' for x in model_params])\n",
        "    plt.grid(True, which='both', linestyle='-', linewidth=0.5)\n",
        "    plt.ylim(50, 100)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_saycan_llm_scaling()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Parameter Exploration\n",
        "\n",
        "Let's create an interactive widget to explore the SayCan decision logic. You can adjust the LLM's confidence and the robot's affordance for a specific skill and see how it affects the final choice. This demonstrates the balance between \"what makes sense\" (language) and \"what is possible\" (affordance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_saycan_explorer(llm_pick_apple=0.8, affordance_pick_apple=0.1):\n",
        "    \"\"\"\n",
        "    Interactive widget to explore the SayCan scoring.\n",
        "    \"\"\"\n",
        "    # Other skills have fixed scores for this example\n",
        "    scores = {\n",
        "        'find apple': {'llm': 0.9, 'affordance': 0.7},\n",
        "        'pick apple': {'llm': llm_pick_apple, 'affordance': affordance_pick_apple},\n",
        "        'place on_table': {'llm': 0.3, 'affordance': 0.05} # can't place if hand is empty\n",
        "    }\n",
        "    \n",
        "    combined_scores = {skill: data['llm'] * data['affordance'] for skill, data in scores.items()}\n",
        "    best_action = max(combined_scores, key=combined_scores.get)\n",
        "    \n",
        "    df = pd.DataFrame(scores).T\n",
        "    df.columns = ['LLM Score', 'Affordance Score']\n",
        "    df['Combined Score'] = df['LLM Score'] * df['Affordance Score']\n",
        "    df = df.sort_values(by='Combined Score', ascending=False)\n",
        "    \n",
        "    print(\"--- SayCan Interactive Decision Matrix ---\")\n",
        "    display(df)\n",
        "    print(f\"==> Best Action: {best_action}\")\n",
        "    print(\"\\nTry lowering the affordance for 'pick apple' to see when 'find apple' becomes the better choice.\")\n",
        "\n",
        "\n",
        "interact(\n",
        "    interactive_saycan_explorer,\n",
        "    llm_pick_apple=FloatSlider(min=0.0, max=1.0, step=0.05, value=0.8, description='LLM Score (pick apple)'),\n",
        "    affordance_pick_apple=FloatSlider(min=0.0, max=1.0, step=0.05, value=0.9, description='Affordance (pick apple)')\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Research Context & Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Contribution in Context\n",
        "\n",
        "The body of work presented in the lecture represents a significant contribution by championing a specific, scalable paradigm for robot learning. It places itself firmly in the \"data-driven\" camp, moving away from classical robotics and even the dominant online RL approaches of the recent past.\n",
        "\n",
        "- **Historical Context:** The lecture traces the team's journey from the \"arm farm\" (on-policy RL, 2016), through various methods like QT-Opt and BC-Zero, to a consolidation around multi-task imitation learning (2022). This history highlights the \"bitter lessons\" learned: methods that scale with data and general computation (like Transformers on large offline datasets) ultimately win out over more complex, specialized algorithms.\n",
        "- **The Bitter Lesson 2.0:** A key contextual idea is Karol Hausman's \"Bitter Lesson 2.0,\" which posits that researchers should leverage methods that can utilize improvements in foundation models. SayCan and Inner Monologue are prime examples, as their performance is directly tied to the quality of the underlying LLMs and VLMs, which are improving at a staggering rate externally.\n",
        "- **Language as the Universal API:** The central theme is using natural language as the glue connecting different components (human intent, high-level planning, low-level skills, visual feedback). This makes the system more modular and allows each part to be improved independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Current Research Directions Mentioned\n",
        "\n",
        "The lecture hints at several ongoing and future research directions:\n",
        "\n",
        "1.  **Scaling Data Collection:** The current bottleneck is the acquisition of large, diverse, real-world robotics data. While DIAL helps get more out of existing data, the ultimate goal is to find cheaper, more autonomous ways to collect new data beyond human teleoperation.\n",
        "2.  **Overcoming Inference Speed Bottlenecks:** RT-1 was constrained to 35M parameters to run at 3Hz. A major open question is how to run much larger, more capable foundation models on real robots without sacrificing reactivity. This might involve techniques like model distillation, quantization, or new hardware.\n",
        "3.  **Bringing RL Back In:** The current approach is based on imitation learning. A future step is to use this strong imitation policy as a starting point for further improvement with offline reinforcement learning, allowing the robot to learn from autonomous experience and surpass the initial human demonstrators.\n",
        "4.  **Improving Long-Context Understanding:** The current RT-1 model uses a history of only 6 images (2 seconds). For tasks that take minutes, this is insufficient. Future work will need to explore architectures that can handle much longer contexts, perhaps using retrieval-based methods or recurrent state management.\n",
        "5.  **Embodied Action Improving LLMs:** The relationship could be symbiotic. While foundation models help robots, large-scale datasets of embodied, causal interactions from robots could be used to improve the reasoning and grounding of future LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Applications\n",
        "\n",
        "While still in the research phase, the practical application driving this work is the development of general-purpose robots that can operate usefully in human environments. The choice of the \"micro-kitchen\" domain is intentional, as it contains a wide variety of objects, tasks, and semantic concepts relevant to everyday human life.\n",
        "\n",
        "The ultimate goal is to move beyond single-task, factory-floor robots to create assistants that can understand high-level, ambiguous natural language commands and execute them robustly in the chaotic real world. The projects discussed are concrete steps toward building the foundational components—robust perception, scalable skill learning, and common-sense planning—required for such an application."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
